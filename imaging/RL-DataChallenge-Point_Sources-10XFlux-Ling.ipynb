{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to point source imaging with COSIpy-classic\n",
    "In this notebook, we'll use a Richardson-Lucy deconvolution algorithm to image four point sources: the Crab, Cyg X-1, Cen A, and Vela. This analysis requires significant computer memory (>50 GB), so you may want to use a more resource-intensive computer for this work. Please refer to the README for additional information on each step of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "We will need to import the cosipy-classic functions from COSIpy_dc1.py, response_dc1, and COSIpy_tools_dc1, as well as some other standard Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from COSIpy_dc1 import *\n",
    "import response_dc1\n",
    "from COSIpy_tools_dc1 import *\n",
    "from tqdm.autonotebook import tqdm\n",
    "from numba import set_num_threads\n",
    "\n",
    "import pickle\n",
    "\n",
    "# set parallelism for whole notebook\n",
    "set_num_threads(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the modified RL algorithm implemented here, we need to define a jaxopt objective function that fits background plus two images (the current image plus a delta image given by the RL formalism)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.config\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy.stats as jstats\n",
    "import jaxopt\n",
    "\n",
    "# to better match Stan's behavior\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# objective function for MLE\n",
    "def objective(params, data):\n",
    "        (Abg, flux) = params\n",
    "        (expc, cdelta, bg_model, bg_idx_arr, y, mu_flux, sigma_flux, mu_Abg, sigma_Abg) = data\n",
    " \n",
    "        M = Abg[bg_idx_arr[:,None]] * bg_model + flux[0] * expc + flux[1] * cdelta\n",
    "\n",
    "        # ensure that we don't accidentally use negative Possion means, which blows up likelihood\n",
    "        M = jnp.maximum(M, 0)\n",
    "        \n",
    "        lp = jnp.sum(jstats.poisson.logpmf(y, M), axis=None) + \\\n",
    "             jnp.sum(jstats.norm.logpdf(flux, mu_flux, sigma_flux)) + \\\n",
    "             jnp.sum(jstats.norm.logpdf(Abg, mu_Abg, sigma_Abg))\n",
    "        \n",
    "        return -lp  # minimize to maximize LL\n",
    "\n",
    "#opt = { 'disp': True }\n",
    "optimizer = jaxopt.ScipyBoundedMinimize(fun=objective, method=\"l-bfgs-b\", tol=1e-10)#, options=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define file names\n",
    "The 'Point_sources_10x_BG.tra.gz' file contains simulations of all four point sources, each at 10X their true flux, and Ling background. This is our data file.\n",
    "\n",
    "You can optionally image only the point sources (without background) by changing this file to point source-only simulation. You will have to adjust the RL algorithm parameters later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data_products' # directory containing data & response files\n",
    "filename = 'Point_sources_10x_BG.tra.gz' # combined simulation\n",
    "response_filename = data_dir + '/Continuum_imaging_response.npz' # detector response\n",
    "background_filename = data_dir + '/Scaled_Ling_BG_1x.npz' # background response\n",
    "background_mode = 'from file'\n",
    "\n",
    "pklfname = \"ptsrc.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read simulation and define analysis object\n",
    "Read in the data set and create the main cosipy-classic â€œanalysis1\" object, which provides various functionalities to study the specified file. This cell usually takes a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    analysis1 = pickle.load(open(pklfname,'rb'))\n",
    "    \n",
    "except:\n",
    "    print(\"loading analysis dataset\")\n",
    "    analysis1 = COSIpy(data_dir, filename)\n",
    "    analysis1.read_COSI_DataSet()\n",
    "    with open(pklfname, 'wb') as f:\n",
    "        pickle.dump(analysis1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bin the data\n",
    "Calling \"get_binned_data()\" may take several minutes, depending on the size of the dataset and the number of bins. Keep an eye on memory here: if your time bins are very small, for example, this could be an expensive operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bin sizes\n",
    "Delta_T = 1800 # time bin size in seconds\n",
    "energy_bin_edges = np.array([150, 220, 325, 480, 520, 765, 1120, 1650, 2350, 3450, 5000]) # as defined in the response\n",
    "pixel_size = 6. # as defined in the response\n",
    "\n",
    "analysis1.dataset.time_binning_tags_fast(time_bin_size=Delta_T)\n",
    "analysis1.dataset.init_binning(energy_bin_edges=energy_bin_edges, pixel_size=pixel_size) # initiate the binning\n",
    "analysis1.dataset.get_binned_data_fast() # bin data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the shape of the binned data.\n",
    "The binned data are contained in \"analysis1.dataset.binned_data.\" This is a 4-dimensional object representing the 5 dimensions of the Compton data space: (time, energy, $\\phi$, FISBEL).\n",
    "\n",
    "The number of bins in each dimension are shown by calling \"shape.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time, energy, phi, fisbel\")\n",
    "print(analysis1.dataset.binned_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can print the width of each time bin and the total time\n",
    "print(analysis1.dataset.times.times_wid)\n",
    "print(analysis1.dataset.times.total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot raw spectrum & light curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis1.dataset.plot_lightcurve()\n",
    "\n",
    "analysis1.dataset.plot_raw_spectrum()\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the pointing object with the cosipy pointing class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of pointings (balloon stability + Earth rotation)\n",
    "pointing1 = Pointing(dataset=analysis1.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the path of the Crab through the field-of-view.\n",
    "This isn't necessary for the imaging algorithm, but is illustrative in the case of a point source. Take the Crab as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_crab, b_crab = 184.55746, -5.78436 # Location of the Crab in Galactic coordinates [deg]\n",
    "\n",
    "source = 'Crab Nebula'\n",
    "\n",
    "plt.plot(np.rad2deg(pointing1.zpoins)[:,0]+360, np.rad2deg(pointing1.zpoins[:,1]), 'ko', label='COSI zenith pointing')\n",
    "plt.plot(l_crab, b_crab, '*r', markersize=10 , label=f'{source}')\n",
    "plt.xlabel('Longitude [deg]')\n",
    "plt.ylabel('Latitude [deg]')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COSI's field of view extends ~60 deg from zenith, hence why the Zenith is labeled as +60 deg above the horizon\n",
    "analysis1.plot_elevation([l_crab], [b_crab], [f'{source}'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the BG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ling BG simulation to model atmospheric background\n",
    "background1 = BG(dataset=analysis1.dataset,mode=background_mode,filename=background_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the Response Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuum response\n",
    "rsp = response_dc1.SkyResponse(filename=response_filename,pixel_size=pixel_size) # read in detector response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the shape of the data space\n",
    "The shape of the response spans (Galactic latitude $b$, Galactic longitude $\\ell$, Compton scattering angle $\\phi$,  FISBEL, energy). There is 1 energy bin for the 511 keV response (\"analysis1.dataset.energies.n_energy_bins\"). This is why there is no fifth dimension for the energy printed below. The shape of the data and background objects span (time, Compton scattering angle, FISBEL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp.rsp.response_grid_normed_efinal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.shape(analysis1.dataset.binned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(background1.bg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a grid on the sky to make images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our sky-grid on a regular (pixel_size x pixel_size) grid for testing (later finer grid)\n",
    "binsize = np.deg2rad(pixel_size)\n",
    "\n",
    "# Number of pixels in l and b\n",
    "n_l = int(360/pixel_size)\n",
    "n_b = int(180/pixel_size)\n",
    "\n",
    "# Galactic coordiantes: l and b pixel edges\n",
    "l_arrg = np.linspace(-np.pi,   np.pi,   n_l+1)\n",
    "b_arrg = np.linspace(-np.pi/2, np.pi/2, n_b+1)\n",
    "\n",
    "# Making a grid\n",
    "L_ARRg, B_ARRg = np.meshgrid(l_arrg, b_arrg)\n",
    "\n",
    "# Choosing the centre points as representative\n",
    "l_arr = l_arrg[0:-1] + binsize/2\n",
    "b_arr = b_arrg[0:-1] + binsize/2\n",
    "L_ARR, B_ARR = np.meshgrid(l_arr, b_arr)\n",
    "\n",
    "# Define solid angle for each pixel for normalisations later\n",
    "domega    = binsize * np.diff(np.sin(b_arrg))  # per latitude\n",
    "domegaMap = domega[:,None] # permit computing \"map / domega\" on n_b x n_l map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert sky grid to zenith/azimuth pairs for all pointings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the zeniths and azimuths on that grid for all times\n",
    "coords = zenaziGrid_fast(pointing1.ypoins,\n",
    "                         pointing1.xpoins,\n",
    "                         pointing1.zpoins,\n",
    "                         L_ARR.ravel(), B_ARR.ravel(),\n",
    "                         pixel_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for next routines ... \n",
    "coords = coords.reshape(n_b, n_l, pointing1.dtpoins.size, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get observation indices for non-zero bins\n",
    "Here we also chose an energy bin to image. Energy bin \"2\" in the continuum response (and, necessarily, \"energy_bin_edges\" at the beginning of the notebook) is $320-480$ keV. We choose this energy bin because it has the highest count rate. Refer to the energy spectrum generated earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an energy bin to analyze\n",
    "ebin = 2 # Analyzing 320-480 keV \n",
    "nonzero_idx = background1.calc_this[ebin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the response dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_response_CDS = rsp.rsp.response_grid_normed_efinal.reshape(\n",
    "    n_b,\n",
    "    n_l,\n",
    "    analysis1.dataset.phis.n_phi_bins*\\\n",
    "    analysis1.dataset.fisbels.n_fisbel_bins, analysis1.dataset.energies.n_energy_bins)[:, :, nonzero_idx, ebin]\n",
    "\n",
    "sky_response_CDS = np.ascontiguousarray(sky_response_CDS) # to speed up scaled response calculation\n",
    "del rsp  # get rid of no-longer-needed full response, which is much larger than sky_response_CDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced response dimensions:\n",
    "# lat x lon x CDS\n",
    "sky_response_CDS.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the general response for the current data set\n",
    "This has to be done only once (for the data set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import get_image_response_from_pixelhit_general\n",
    "\n",
    "sky_response_scaled = [] # clear out any old (large!) matrix if we are running this more than once\n",
    "\n",
    "cut = 90 \n",
    "\n",
    "sky_response_scaled = get_image_response_from_pixelhit_general(\n",
    "    Response=sky_response_CDS,\n",
    "    coords = coords,\n",
    "    dt=pointing1.dtpoins,\n",
    "    times_min=analysis1.dataset.times.times_min,\n",
    "    n_ph_dx=analysis1.dataset.times.n_ph_dx,\n",
    "    domega=domega,\n",
    "    n_hours=analysis1.dataset.times.n_ph,\n",
    "    pixel_size=pixel_size,\n",
    "    cut=cut)\n",
    "    #altitude_correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data-set-specific response dimensions\n",
    "# times x lat x lon x CDS\n",
    "sky_response_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exposure map\n",
    "i.e. the response weighted by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import emap_fast\n",
    "\n",
    "expo_map = emap_fast(sky_response_scaled, n_b, n_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the exposure map weighted with the pixel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(projection='aitoff')\n",
    "p = plt.pcolormesh(L_ARRg,B_ARRg,np.roll(expo_map/domegaMap,axis=1,shift=0))\n",
    "plt.contour(L_ARR,B_ARR,np.roll(expo_map/domegaMap,axis=1,shift=0),colors='black')\n",
    "plt.colorbar(p, orientation='horizontal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the RL algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for a starting map for the RL deconvolution. We choose an isotropic map, i.e. all pixels on the sky are initialized with the same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsoMap(n_b, n_l, A0, domega):\n",
    "    norm  = n_l * np.sum(domega)\n",
    "    return A0/norm * np.ones((n_b, n_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of time bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2h = analysis1.dataset.binned_data.shape[0]\n",
    "d2h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only one energy bin (as above) for data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ebin: ',ebin)\n",
    "dataset = analysis1.dataset.binned_data[:,ebin,:,:].reshape(d2h,\n",
    "                                                            analysis1.dataset.phis.n_phi_bins*analysis1.dataset.fisbels.n_fisbel_bins)[:,nonzero_idx]\n",
    "dataset = np.ascontiguousarray(dataset) # for performance in R-L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same for background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_model = background1.bg_model_reduced[ebin]\n",
    "background_model = np.ascontiguousarray(background_model) # for performance in R-L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for consistency of data and background\n",
    "They must have the same dimensions. If not, the algorithm won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape, background_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set an initial guess for the background amplitude\n",
    "Feel free to play with this value, but here are suggestions informed by testing thus far:\n",
    "\n",
    "### If source+BG:\n",
    "We suggest setting \"fitted_bg\" to 0.9 or 0.99 when the loaded data/simulation (analysis1 object) contains both source and background. This is a rough estimate of the background contribution (90, 99%) to the entire data set.\n",
    "\n",
    "### If analyzing source only:\n",
    "When the analysis1 object does not contain background, we suggest setting this parameter to 1E-6, i.e. very close to zero background contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_bg = np.array([0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Richardson-Lucy algorithm\n",
    "\n",
    "## Individual steps are explained in the code.\n",
    "The steps follow the algorithm as outlined in [Knoedlseder et al. 1999](https://ui.adsabs.harvard.edu/abs/1999A%26A...345..813K/abstract). Refer to that paper for a mathematical description of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might not use this depending on if you choose to smooth the delta map\n",
    "\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import convolve_fast, convdelta_fast\n",
    "#import time\n",
    "\n",
    "# Experiment with these variables!\n",
    "#############################\n",
    "# initial map (isotropic flat, small value)\n",
    "map_init = IsoMap(n_b, n_l, 0.01, domega)\n",
    "\n",
    "# number of RL iterations\n",
    "maxiters = 150\n",
    "\n",
    "# if MAP likelihood changes by less than this fraction in 1 iteration, terminate\n",
    "ltol = 1e-9\n",
    "\n",
    "# acceleration parameter\n",
    "afl_scl = 2000.\n",
    "\n",
    "# Define regions of the sky that we actually cannot see\n",
    "# here we select everything, i.e. we have no bad exposure\n",
    "\n",
    "bad_expo = np.where(expo_map/domegaMap <= 0)\n",
    "\n",
    "#############################\n",
    "\n",
    "######################################\n",
    "# Initial sky map setup from map_init\n",
    "######################################\n",
    "\n",
    "# Current map starts as initial map\n",
    "curr_map = map_init\n",
    "\n",
    "# setting the map to zero where we selected a bad exposure (we didn't, but to keep it general)\n",
    "curr_map[bad_expo] = 0\n",
    "\n",
    "# check for each pixel to be finite (must be true for map_init)\n",
    "assert not np.isnan(curr_map).any(), \"NaNs in initial map!\"\n",
    "\n",
    "# convolve this map with the response\n",
    "#print('Convolving with response (init expectation)')\n",
    "#tstart = time.time()\n",
    "curr_expectation = convolve_fast(sky_response_scaled, curr_map,\n",
    "                                 sky_response_scaled.shape[0], sky_response_scaled.shape[3],\n",
    "                                 n_b, n_l)\n",
    "#tend = time.time()\n",
    "#print(f'Time in convolution: {tend - tstart:.2f}s')\n",
    "\n",
    "#########################################################\n",
    "# Computations pulled out of loop\n",
    "#########################################################\n",
    "\n",
    "## Define background model cuts, indices, and resulting number of cuts\n",
    "bg_cuts, idx_arr, Ncuts = background1.bg_cuts, background1.idx_arr, background1.Ncuts\n",
    " \n",
    "# temporary background model\n",
    "model_bg = background_model * fitted_bg[idx_arr, None]\n",
    "\n",
    "# cf. Knoedlseder+1997 what the values denominator etc are\n",
    "# this is the response R summed over the CDS and the time bins\n",
    "\n",
    "# denominator scaled by fourth root to avoid exposure e#dge effects\n",
    "# You can try changing 0.25 to 0, 0.5, for example\n",
    "den_scale = 0.25\n",
    "\n",
    "idenominator = expo_map**(-(1-den_scale))\n",
    "\n",
    "#########################################################\n",
    "# Storage for intermediate parameters\n",
    "#########################################################\n",
    "\n",
    "# maps per iteration\n",
    "map_iterations = np.empty((maxiters, n_b, n_l))\n",
    "\n",
    "# likelihood of maps (vs. initial, i.e., basically only background)\n",
    "map_likelihoods = np.empty(maxiters)\n",
    "\n",
    "# store per-iter fit likelihoods, i.e., fit quality\n",
    "intermediate_lp = np.empty(maxiters)\n",
    "\n",
    "# store per-iter acceleration parameters (lambda)\n",
    "acc_par = np.empty(maxiters)\n",
    "\n",
    "# store per-iter fitted background parameters \n",
    "bg_pars = np.empty((maxiters, Ncuts))\n",
    "\n",
    "###########################################################\n",
    "## iterative R-L loop                    \n",
    "###########################################################\n",
    "\n",
    "# expectation (in data space) is the image (curr_expectation) plus the background (model_bg)\n",
    "curr_expectation_tot = curr_expectation + model_bg \n",
    "\n",
    "# save initial map\n",
    "map_iterations[0,:,:] = curr_map\n",
    "\n",
    "# save initial map's likelihood\n",
    "map_likelihoods[0] = cashstat(dataset, curr_expectation_tot)\n",
    "\n",
    "for its in tqdm(range(1, maxiters)):\n",
    "\n",
    "    # calculate numerator of RL algorithm\n",
    "   \n",
    "    #print(f'Calculating Delta image, iteration {its}, numerator')\n",
    "    #tstart = time.time()\n",
    "    numerator = convdelta_fast(sky_response_scaled, dataset, curr_expectation_tot,\n",
    "                               n_b, n_l, dataset.shape[0], dataset.shape[1])\n",
    "    #tend = time.time()\n",
    "    #print(f'Time in Delta image calc: {tend - tstart:.2f}s')\n",
    "    \n",
    "    delta_map_tot = curr_map * numerator * idenominator\n",
    "   \n",
    "    # You can also try to smooth the delta map\n",
    "    #curr_delta_map_tot = gaussian_filter(curr_delta_map_tot, 0.5)\n",
    "        \n",
    "    # zero our bad exposure regions\n",
    "    delta_map_tot[bad_expo] = 0\n",
    "\n",
    "    # should never happen\n",
    "    delta_map_tot[np.isnan(delta_map_tot)] = 0\n",
    "            \n",
    "    # convolve delta image\n",
    "    #print(f'Convolving Delta image, iteration {its}')\n",
    "    #tstart = time.time()\n",
    "    conv_delta_map_tot = convolve_fast(sky_response_scaled, delta_map_tot,\n",
    "                                    sky_response_scaled.shape[0], sky_response_scaled.shape[3],\n",
    "                                    n_b, n_l)\n",
    "    #tend = time.time()\n",
    "    #print(f'Time in convolution: {tend - tstart:.2f}s')\n",
    "\n",
    "    # Find maximum acceleration parameter to multiply delta image with\n",
    "    # so that the total image is still positive everywhere.\n",
    "    # If there are no negative entries in delta_map_tot_old, there is no upper bound on the\n",
    "    # acceleration.  Original code used a value of ~10000 in this case.  If we use much larger\n",
    "    # value, RL seems to oscillate rather than converging smoothly and gives a worse final\n",
    "    # likelihood (observed on the Point_Sources notebook, which is the only one with this issue).\n",
    "    assert np.min(curr_map) >= 0, \"current map contains negative entries!\"\n",
    "    neg = delta_map_tot < 0\n",
    "    if not neg.any():\n",
    "        afl = 10000\n",
    "    else:\n",
    "        afl = int(np.floor(np.min(-afl_scl * curr_map[neg] / delta_map_tot[neg]))) \n",
    "        afl = min(afl, 10000)\n",
    "    \n",
    "    print('Maximum acceleration parameter found: ', afl/afl_scl)\n",
    "\n",
    "    # fit:\n",
    "    \n",
    "    cdelta = conv_delta_map_tot/afl_scl\n",
    "\n",
    "    mu_Abg = fitted_bg    # can play with this\n",
    "    sigma_Abg = fitted_bg # can play with this\n",
    "    mu_flux = np.array([1,afl/2])\n",
    "    sigma_flux = np.array([1e-2,afl])\n",
    "\n",
    "    init_params =  (jnp.ones(Ncuts) * fitted_bg, jnp.array([1, afl/2.]))\n",
    "    \n",
    "    acceleration_factor_limit = afl * 0.95\n",
    "    lower_bounds = (jnp.ones(Ncuts) * 1e-8,    jnp.ones(2) * 1e-8)\n",
    "    upper_bounds = (jnp.ones(Ncuts) * jnp.inf, jnp.ones(2) * acceleration_factor_limit)\n",
    "    \n",
    "    #print('Optimizing bg parameters')\n",
    "    #tstart = time.time()\n",
    "    res = optimizer.run(init_params, bounds=(lower_bounds, upper_bounds),\n",
    "                        data=(curr_expectation, cdelta, model_bg, idx_arr, dataset,\n",
    "                              mu_flux, sigma_flux, mu_Abg, sigma_Abg))\n",
    "    #tend = time.time()\n",
    "    #print(f'Time in optimizer: {tend - tstart:.2f}s')\n",
    "\n",
    "    if not res.state.success:\n",
    "        print(\"*** Optimizer failed! rerun with options = { 'disp': True } to see error messages\")\n",
    "       \n",
    "        # proceed with a safe acceleration <= 1 (safe = new map does not go negative at any pixel)\n",
    "        print(\"proceeding with a safe acceleration parameter\")\n",
    "        accScale = np.minimum(1., acceleration_factor_limit)\n",
    "    else:\n",
    "        # save values\n",
    "        #print(f'Saving new map, and fitted parameters, iteration {its}')\n",
    "        intermediate_lp[its-1] = -res.state.fun_val\n",
    "        \n",
    "        newAbg, newflux = res.params\n",
    "        newAcc = newflux[1]\n",
    "        bg_pars[its-1,:] = newAbg\n",
    "        acc_par[its-1]   = newAcc\n",
    "\n",
    "        accScale = float(newAcc)/afl_scl\n",
    "    \n",
    "    # plot each iteration's map and its delta map here to match previous impl's behavior\n",
    "    # (not required, but nice to see how the algorithm is doing)\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(121)\n",
    "    plt.pcolormesh(L_ARRg,B_ARRg,np.roll(curr_map, axis=1, shift=0)) \n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.pcolormesh(L_ARRg,B_ARRg,np.roll(delta_map_tot, axis=1, shift=0)) \n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    # make new map as old map plus scaled delta map\n",
    "    curr_map += accScale * delta_map_tot\n",
    "    \n",
    "    # setting the map to zero where we selected a bad exposure (we didn't, but to keep it general)\n",
    "    curr_map[bad_expo] = 0\n",
    "    \n",
    "    # check for each pixel to be finite\n",
    "    curr_map[np.isnan(curr_map)] = 0\n",
    "\n",
    "    # save map\n",
    "    map_iterations[its,:,:] = curr_map \n",
    "\n",
    "    # make new expectation as old expectation plus scaled conv_delta map\n",
    "    curr_expectation += accScale * conv_delta_map_tot\n",
    "\n",
    "    # expectation (in data space) is the image (expectation) plus the background (model_bg)\n",
    "    curr_expectation_tot = curr_expectation + model_bg \n",
    "\n",
    "    # calculate likelihood of current total expectation\n",
    "    map_likelihoods[its] = cashstat(dataset, curr_expectation_tot)\n",
    "\n",
    "    # how much did the MAP likelihood improve since the prior iteration?\n",
    "    dml = np.abs((map_likelihoods[its] - map_likelihoods[its-1])/map_likelihoods[its-1])\n",
    "\n",
    "    print(f\"After iteration {its}: MAP likelihood = {map_likelihoods[its]:.2f}, rel. change = {dml:.2e}\")\n",
    "\n",
    "    if dml < ltol:\n",
    "        print(f\"MAP likelihood change was less than {ltol} -- terminating\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the fitted background parameter and the map flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(range(its), [i[0] for i in bg_pars[:its]], '.-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('BG params]')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "map_fluxes = np.zeros(its+1)\n",
    "for i in range(its+1):\n",
    "    map_fluxes[i] = np.sum(map_iterations[i,:,:]*domegaMap)\n",
    "    \n",
    "plt.plot(map_fluxes[:its],'o-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Flux')# [ph/keV]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did the algorithm converge? Look at the likelihoods.\n",
    "intermediate_lp: Fit likelihoods, i.e. fit quality\n",
    "\n",
    "map_likelihoods: likelihood of maps (vs. initial i.e. basically only background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(np.arange(its), intermediate_lp[:its], '.-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('likelihood (intermediate_lp)')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(range(its+1), map_likelihoods[:its+1], '.-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('likelihood (map_likelihoods)')\n",
    "\n",
    "print(f'final MAP likelihood = {map_likelihoods[its]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the image!\n",
    "You can loop over all iterations to make a GIF or just show one iteration (usually the final iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import Video\n",
    "\n",
    "from matplotlib import animation\n",
    "\n",
    "from matplotlib import colors\n",
    "\n",
    "from scipy.ndimage import gaussian_filter as smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an image to plot\n",
    "idx = its  # final image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg2rad = np.pi/180\n",
    "\n",
    "# Choose a color map like viridis (matplotlib default), nipy_spectral, twilight_shifted, etc. Not jet.\n",
    "cmap = plt.get_cmap('viridis') \n",
    "\n",
    "# Bad exposures will be gray\n",
    "cmap.set_bad('lightgray')\n",
    "\n",
    "\n",
    "##################\n",
    "# Select here which pixels should be gray\n",
    "map_iterations_nan = np.copy(map_iterations)\n",
    "\n",
    "# Select also non-zero exposures here to be gray (avoiding the edge effects)\n",
    "# You can play with this. Most success in testing with 1e4, 1e3\n",
    "bad_expo = np.where(expo_map/domegaMap <= 1e4) \n",
    "\n",
    "map_iterations_nan[:, bad_expo[0], bad_expo[1]] = np.nan\n",
    "#################    \n",
    "\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(10.24,7.68), subplot_kw={'projection':'aitoff'}, nrows=1, ncols=1)\n",
    "\n",
    "ax.set_xticks(np.array([-120,-60,0,60,120])*deg2rad)\n",
    "ax.set_xticklabels([r'$-120^{\\circ}$'+'\\n',\n",
    "                            r'$-60^{\\circ}$'+'\\n',\n",
    "                            r'$0^{\\circ}$'+'\\n',\n",
    "                            r'$60^{\\circ}$'+'\\n',\n",
    "                            r'$120^{\\circ}$'+'\\n'])\n",
    "ax.tick_params(axis='x', colors='orange')\n",
    "\n",
    "ax.set_yticks(np.array([-60,-30,0,30,60])*deg2rad)\n",
    "ax.tick_params(axis='y', colors='orange')\n",
    "\n",
    "plt.xlabel('Gal. Lon. [deg]')\n",
    "plt.ylabel('Gal. Lat. [deg]')\n",
    "\n",
    "\n",
    "# \"ims\" is a list of lists, each row is a list of artists to draw in the\n",
    "# current frame; here we are just animating one artist, the image, in\n",
    "# each frame\n",
    "ims = []\n",
    "\n",
    "# If you want to make a GIF of all iterations:\n",
    "#for i in range(its+1):\n",
    "\n",
    "# If you only want to plot one image:\n",
    "for i in [idx]:\n",
    "\n",
    "    ttl = plt.text(0.5, 1.01, f'RL iteration {i}', horizontalalignment='center', \n",
    "                   verticalalignment='bottom', transform=ax.transAxes)\n",
    "    \n",
    "    # Either gray-out bad exposure (map_iterations_nan) or don't mask (map_iterations)\n",
    "    # Masking out bad exposure \n",
    "    image = map_iterations_nan[i,:,:]\n",
    "    #image = map_iterations[i,:,:]\n",
    "\n",
    "    img = ax.pcolormesh(L_ARRg,B_ARRg,\n",
    "                        \n",
    "                        # Can shift the image along longitude. Here, no shift.\n",
    "                        np.roll(image, axis=1, shift=0),\n",
    "            \n",
    "                        # Optionally smooth with gaussian filter\n",
    "                        #smooth(np.roll(image, axis=1, shift=0), 0.75/pixel_size),\n",
    "                        \n",
    "                        cmap=plt.cm.viridis,\n",
    "                        \n",
    "                        # Optionally set the color scale. Default: linear\n",
    "                        #norm=colors.PowerNorm(0.33)\n",
    "                       )\n",
    "    ax.grid()\n",
    "    \n",
    "    ims.append([img, ttl])\n",
    "\n",
    "cbar = fig.colorbar(img, orientation='horizontal')\n",
    "cbar.ax.set_xlabel(r'[Arbitrary Units]')\n",
    "    \n",
    "\n",
    "# Can save a sole image as a PDF \n",
    "#plt.savefig(data_dir + f'images/511keV_RL_image_iteration{idx}.pdf', bbox_inches='tight')\n",
    "\n",
    "# # Can save all iterations as a GIF\n",
    "# ani = animation.ArtistAnimation(fig, ims, interval=200, blit=True, repeat_delay=0)\n",
    "# ani.save(f'/home/jacqueline/511keV_RL_image_{idx}iterations.gif')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do we see?\n",
    "The Crab nebula is the only easily visible source in this combined simulation of 10x flux Crab, 10x flux Cygnus X-1, 10x flux Centaurus A, 10x Vela, and 1x flux Ling background (scaled to the observed 2016 flight background level). \n",
    "\n",
    "You can play with the color scaling to try to enhance the appearance of the other sources. Vela is likely too dim to be seen, however. \n",
    "\n",
    "You can also try running this notebook without including the Ling background. Change the loaded .tra.gz file at the beginning, adjust RL parameters as necessary, and see if the four point sources are more easily resolved without background contamination!\n",
    "\n",
    "As another suggestion, what happens if you run this notebook using the 511 keV response? The 1809 keV response? A different energy bin of the continuum response? \n",
    "\n",
    "You can try combining these four point sources and Ling BG with the 10x 511 keV and 10x $^{26}$Al simulations for a full combined imaging test, using all three response simulations too."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-cosi-python-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
