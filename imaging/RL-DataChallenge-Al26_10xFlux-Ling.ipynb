{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to $^{26}$Al imaging with COSIpy-classic\n",
    "In this notebook, we'll use a Richardson-Lucy deconvolution algorithm to image $^{26}$Al emission from the Milky Way Galaxy. This analysis requires significant computer memory (>50 GB), so you may want to use a more resource-intensive computer for this work. Please refer to the README for additional information on each step of the analysis.\n",
    "\n",
    "**Note as of January 20, 2023**: This notebook was tested and executes in its current form on Ubuntu 20.04. We're hearing reports that the image deconvolution fails at early iterations (~10) on Mac M1 and does not run on Ubuntu 22.04, the latter of which cannot support pystan. This is under investigation. Please attempt this notebook as you wish, try changing parameters in the algorithm, or refer to the 511 keV imaging notebook for another example of diffuse imaging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "We will need to import the cosipy-classic functions from COSIpy_dc1.py, response_dc1, and COSIpy_tools_dc1, as well as some other standard Python packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from COSIpy_dc1 import *\n",
    "import response_dc1\n",
    "from COSIpy_tools_dc1 import *\n",
    "from tqdm.autonotebook import tqdm\n",
    "from numba import set_num_threads\n",
    "\n",
    "import pickle\n",
    "\n",
    "# set parallelism for whole notebook\n",
    "set_num_threads(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the modified RL algorithm implemented here, we need to define a jaxopt objective function that fits background plus two images (the current image plus a delta image given by the RL formalism)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.config\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy.stats as jstats\n",
    "import jaxopt\n",
    "\n",
    "# to better match Stan's behavior\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# objective function for MLE\n",
    "def objective(params, data):\n",
    "        (Abg, flux) = params\n",
    "        (conv_sky, bg_model, bg_idx_arr, y, mu_flux, sigma_flux, mu_Abg, sigma_Abg) = data\n",
    " \n",
    "        M = Abg[bg_idx_arr[:,None]] * bg_model + jnp.sum(flux[:,None,None] * conv_sky, axis=0)\n",
    "\n",
    "        # ensure that we don't accidentally use negative Possion means, which blows up likelihood\n",
    "        M = jnp.maximum(M, 0)\n",
    "        \n",
    "        lp = jnp.sum(jstats.poisson.logpmf(y, M), axis=None) + \\\n",
    "             jnp.sum(jstats.norm.logpdf(flux, mu_flux, sigma_flux)) + \\\n",
    "             jnp.sum(jstats.norm.logpdf(Abg, mu_Abg, sigma_Abg))\n",
    "        \n",
    "        return -lp  # minimize to maximize LL\n",
    "\n",
    "#opt = { 'disp': True }\n",
    "optimizer = jaxopt.ScipyBoundedMinimize(fun=objective, method=\"l-bfgs-b\", tol=1e-10)#, options=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define file names\n",
    "This file contains the 10X flux 511 keV simulation and Ling BG. \n",
    "\n",
    "You can optionally image only 511 keV (without background) by changing this file to the 511 keV-only simulation. You will have to adjust the RL algorithm parameters later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data_products' # directory containing data & response files\n",
    "filename = 'Al26_10xFlux_and_Ling.inc1.id1.extracted.tra.gz'# Al-26 with Ling BG\n",
    "response_filename = data_dir + '/1809keV_imaging_response.npz' # detector response\n",
    "background_filename = data_dir + '/Scaled_Ling_BG_1x.npz' # background response\n",
    "background_mode = 'from file'\n",
    "\n",
    "# set this to store a pickled version of the dataset to speed up future runs\n",
    "pklfname = 'Al26.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read simulation and define analysis object\n",
    "Read in the data set and create the main cosipy-classic â€œanalysis1\" object, which provides various functionalities to study the specified file. This cell usually takes a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    analysis1 = pickle.load(open(pklfname,'rb'))\n",
    "    \n",
    "except:\n",
    "    print(\"loading analysis dataset\")\n",
    "    analysis1 = COSIpy(data_dir, filename)\n",
    "    analysis1.read_COSI_DataSet()\n",
    "    with open(pklfname, 'wb') as f:\n",
    "        pickle.dump(analysis1, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bin the data\n",
    "Calling \"get_binned_data()\" may take several minutes, depending on the size of the dataset and the number of bins. Keep an eye on memory here: if your time bins are very small, for example, this could be an expensive operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bin sizes\n",
    "Delta_T = 1800 # time bin size in seconds\n",
    "energy_bin_edges = np.array([1803, 1817]) # as defined in the response\n",
    "pixel_size = 6. # as defined in the response\n",
    "\n",
    "analysis1.dataset.time_binning_tags_fast(time_bin_size=Delta_T)\n",
    "analysis1.dataset.init_binning(energy_bin_edges=energy_bin_edges, pixel_size=pixel_size) # initiate the binning\n",
    "analysis1.dataset.get_binned_data_fast() # bin data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the shape of the binned data.\n",
    "The binned data are contained in \"analysis1.dataset.binned_data.\" This is a 4-dimensional object representing the 5 dimensions of the Compton data space: (time, energy, $\\phi$, FISBEL).\n",
    "\n",
    "The number of bins in each dimension are shown by calling \"shape.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time, energy, phi, fisbel\")\n",
    "print(analysis1.dataset.binned_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can print the width of each time bin and the total time\n",
    "print(analysis1.dataset.times.times_wid)\n",
    "print(analysis1.dataset.times.total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot raw spectrum & light curve\n",
    "For a single energy bin, the spectrum is necessarily a top hat in the sole non-zero bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis1.dataset.plot_lightcurve()\n",
    "\n",
    "analysis1.dataset.plot_raw_spectrum()\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the pointing object with the cosipy pointing class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of pointings (balloon stability + Earth rotation)\n",
    "pointing1 = Pointing(dataset=analysis1.dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the BG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ling BG simulation to model atmospheric background\n",
    "background1 = BG(dataset=analysis1.dataset,mode=background_mode,filename=background_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the Response Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26Al response\n",
    "rsp = response_dc1.SkyResponse(filename=response_filename,pixel_size=pixel_size) # read in detector response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the shape of the data space\n",
    "The shape of the response spans (Galactic latitude $b$, Galactic longitude $\\ell$, Compton scattering angle $\\phi$,  FISBEL, energy). There is 1 energy bin for the 511 keV response (\"analysis1.dataset.energies.n_energy_bins\"). This is why there is no fifth dimension for the energy printed below. The shape of the data and background objects span (time, Compton scattering angle, FISBEL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp.rsp.response_grid_normed_efinal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.shape(analysis1.dataset.binned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(background1.bg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a grid on the sky to make images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We define our sky-grid on a regular (pixel_size x pixel_size) grid for testing (later finer grid)\n",
    "binsize = np.deg2rad(pixel_size)\n",
    "\n",
    "# Number of pixels in l and b\n",
    "n_l = int(360/pixel_size)\n",
    "n_b = int(180/pixel_size)\n",
    "\n",
    "# Galactic coordiantes: l and b pixel edges\n",
    "l_arrg = np.linspace(-np.pi,   np.pi,   n_l+1)\n",
    "b_arrg = np.linspace(-np.pi/2, np.pi/2, n_b+1)\n",
    "\n",
    "# Making a grid\n",
    "L_ARRg, B_ARRg = np.meshgrid(l_arrg, b_arrg)\n",
    "\n",
    "# Choosing the centre points as representative\n",
    "l_arr = l_arrg[0:-1] + binsize/2\n",
    "b_arr = b_arrg[0:-1] + binsize/2\n",
    "L_ARR, B_ARR = np.meshgrid(l_arr, b_arr)\n",
    "\n",
    "# Define solid angle for each pixel for normalisations later\n",
    "domega    = binsize * np.diff(np.sin(b_arrg))  # per latitude\n",
    "domegaMap = domega[:,None] # permit computing \"map / domega\" on n_b x n_l map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert sky grid to zenith/azimuth pairs for all pointings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the zeniths and azimuths on that grid for all times\n",
    "zensgrid,azisgrid = zenaziGrid_fast(pointing1.ypoins,\n",
    "                                    pointing1.xpoins,\n",
    "                                    pointing1.zpoins,\n",
    "                                    L_ARR.ravel(), B_ARR.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for next routines ... \n",
    "zensgrid = zensgrid.reshape(n_b, n_l, pointing1.dtpoins.size)\n",
    "azisgrid = azisgrid.reshape(n_b, n_l, pointing1.dtpoins.size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get observation indices for non-zero bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an energy bin to analyze\n",
    "ebin = 0 # We only have one energy bin (1803-1817 keV), so the index is necessarily 0.\n",
    "nonzero_idx = background1.calc_this[ebin]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the response dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_response_CDS = rsp.rsp.response_grid_normed_efinal.reshape(\n",
    "    n_b,\n",
    "    n_l,\n",
    "    analysis1.dataset.phis.n_phi_bins*\\\n",
    "    analysis1.dataset.fisbels.n_fisbel_bins, analysis1.dataset.energies.n_energy_bins)[:, :, nonzero_idx, ebin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced response dimensions:\n",
    "# lat x lon x CDS\n",
    "sky_response_CDS.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the general response for the current data set\n",
    "This has to be done only once (for the data set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import get_image_response_from_pixelhit_general\n",
    "\n",
    "sky_response_scaled = [] # clear out any old (large!) matrix if we are running this more than once\n",
    "\n",
    "cut = 90 \n",
    "\n",
    "sky_response_scaled = get_image_response_from_pixelhit_general(\n",
    "    Response=sky_response_CDS,\n",
    "    zenith=zensgrid,\n",
    "    azimuth=azisgrid,\n",
    "    dt=pointing1.dtpoins,\n",
    "    times_min=analysis1.dataset.times.times_min,\n",
    "    n_ph_dx=analysis1.dataset.times.n_ph_dx,\n",
    "    domega=domega,\n",
    "    n_hours=analysis1.dataset.times.n_ph,\n",
    "    pixel_size=pixel_size,\n",
    "    cut=cut)\n",
    "    #altitude_correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data-set-specific response dimensions\n",
    "# times x lat x lon x CDS\n",
    "sky_response_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exposure map\n",
    "i.e. the response weighted by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import emap_fast\n",
    "\n",
    "expo_map = emap_fast(sky_response_scaled, n_b, n_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the exposure map weighted with the pixel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(projection='aitoff')\n",
    "p = plt.pcolormesh(L_ARRg,B_ARRg,np.roll(expo_map/domegaMap,axis=1,shift=0))\n",
    "plt.contour(L_ARR,B_ARR,np.roll(expo_map/domegaMap,axis=1,shift=0),colors='black')\n",
    "plt.colorbar(p, orientation='horizontal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the RL algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for a starting map for the RL deconvolution. We choose an isotropic map, i.e. all pixels on the sky are initialized with the same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsoMap(n_b, n_l, A0, domega):\n",
    "    norm  = n_l * np.sum(domega)\n",
    "    return A0/norm * np.ones((n_b, n_l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of time bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2h = analysis1.dataset.binned_data.shape[0]\n",
    "d2h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only one energy bin (as above) for data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ebin: ',ebin)\n",
    "dataset = analysis1.dataset.binned_data[:,ebin,:,:].reshape(d2h,\n",
    "                                                            analysis1.dataset.phis.n_phi_bins*analysis1.dataset.fisbels.n_fisbel_bins)[:,nonzero_idx]\n",
    "assert (dataset >= 0).all(), \"dataset contains negative entries!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same for background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_model = background1.bg_model_reduced[ebin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for consistency of data and background\n",
    "They must have the same dimensions. If not, the algorithm won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape, background_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set an initial guess for the background amplitude\n",
    "Feel free to play with this value, but here are suggestions informed by testing thus far:\n",
    "\n",
    "### If source+BG:\n",
    "We suggest setting \"fitted_bg\" to 0.9 or 0.99 when the loaded data/simulation (analysis1 object) contains both source and background. This is a rough estimate of the background contribution (90, 99%) to the entire data set.\n",
    "\n",
    "### If analyzing source only:\n",
    "When the analysis1 object does not contain background, we suggest setting this parameter to 1E-6, i.e. very close to zero background contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_bg = np.array([0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Richardson-Lucy algorithm\n",
    "\n",
    "## Individual steps are explained in the code.\n",
    "The steps follow the algorithm as outlined in [Knoedlseder et al. 1999](https://ui.adsabs.harvard.edu/abs/1999A%26A...345..813K/abstract). Refer to that paper for a mathematical description of the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might not use this depending on if you choose to smooth the delta map\n",
    "\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import convolve_fast, convdelta_fast\n",
    "#import time\n",
    "\n",
    "# Experiment with these variables!\n",
    "#############################\n",
    "# initial map (isotropic flat, small value)\n",
    "map_init = IsoMap(n_b, n_l, 0.01, domega)\n",
    "\n",
    "# number of RL iterations, Usually test with ~50 iterations, and we can get fully converged images with ~150 iterations. \n",
    "maxiters = 78\n",
    "\n",
    "# if MAP likelihood changes by less than this fraction in 1 iteration, terminate\n",
    "ltol = 1e-9\n",
    "\n",
    "# acceleration parameter\n",
    "afl_scl = 2000.\n",
    "\n",
    "# Define regions of the sky that we actually cannot see\n",
    "# here we select everything, i.e. we have no bad exposure\n",
    "\n",
    "bad_expo = np.where(expo_map/domegaMap <= 0)\n",
    "\n",
    "#############################\n",
    "\n",
    "######################################\n",
    "# Initial sky map setup from map_init\n",
    "######################################\n",
    "\n",
    "# Current map starts as initial map\n",
    "curr_map = map_init\n",
    "\n",
    "# setting the map to zero where we selected a bad exposure (we didn't, but to keep it general)\n",
    "curr_map[bad_expo] = 0\n",
    "\n",
    "# check for each pixel to be finite (must be true for map_init)\n",
    "assert not np.isnan(curr_map).any(), \"NaNs in initial map!\"\n",
    "\n",
    "# convolve this map with the response\n",
    "#print('Convolving with response (init expectation)')\n",
    "#tstart = time.time()\n",
    "curr_expectation = convolve_fast(sky_response_scaled, curr_map,\n",
    "                                 sky_response_scaled.shape[0], sky_response_scaled.shape[3],\n",
    "                                 n_b, n_l)\n",
    "#tend = time.time()\n",
    "#print(f'Time in convolution: {tend - tstart:.2f}s')\n",
    "\n",
    "#########################################################\n",
    "# Computations pulled out of loop\n",
    "#########################################################\n",
    "\n",
    "## Define background model cuts, indices, and resulting number of cuts\n",
    "bg_cuts, idx_arr, Ncuts = background1.bg_cuts, background1.idx_arr, background1.Ncuts\n",
    " \n",
    "# temporary background model\n",
    "model_bg = background_model * fitted_bg[idx_arr, None]\n",
    "\n",
    "# cf. Knoedlseder+1997 what the values denominator etc are\n",
    "# this is the response R summed over the CDS and the time bins\n",
    "\n",
    "# denominator scaled by fourth root to avoid exposure e#dge effects\n",
    "# You can try changing 0.25 to 0, 0.5, for example\n",
    "den_scale = 0.25\n",
    "\n",
    "idenominator = expo_map**(-(1-den_scale))\n",
    "\n",
    "# used for background optimization\n",
    "y = dataset.astype(np.int32)\n",
    "\n",
    "#########################################################\n",
    "# Storage for intermediate parameters\n",
    "#########################################################\n",
    "\n",
    "# maps per iteration\n",
    "map_iterations = np.empty((maxiters, n_b, n_l))\n",
    "\n",
    "# likelihood of maps (vs. initial, i.e., basically only background)\n",
    "map_likelihoods = np.empty(maxiters)\n",
    "\n",
    "# store per-iter fit likelihoods, i.e., fit quality\n",
    "intermediate_lp = np.empty(maxiters)\n",
    "\n",
    "# store per-iter acceleration parameters (lambda)\n",
    "acc_par = np.empty(maxiters)\n",
    "\n",
    "# store per-iter fitted background parameters \n",
    "bg_pars = np.empty((maxiters, Ncuts))\n",
    "\n",
    "###########################################################\n",
    "## iterative R-L loop                    \n",
    "###########################################################\n",
    "\n",
    "# expectation (in data space) is the image (curr_expectation) plus the background (model_bg)\n",
    "curr_expectation_tot = curr_expectation + model_bg \n",
    "\n",
    "# save initial map\n",
    "map_iterations[0,:,:] = curr_map\n",
    "\n",
    "# save initial map's likelihood\n",
    "map_likelihoods[0] = cashstat(dataset, curr_expectation_tot)\n",
    "\n",
    "for its in tqdm(range(1, maxiters)):\n",
    "\n",
    "    # calculate numerator of RL algorithm\n",
    "   \n",
    "    #print(f'Calculating Delta image, iteration {its}, numerator')\n",
    "    #tstart = time.time()\n",
    "    numerator = convdelta_fast(sky_response_scaled, dataset, curr_expectation_tot,\n",
    "                               n_b, n_l, dataset.shape[0], dataset.shape[1])\n",
    "    #tend = time.time()\n",
    "    #print(f'Time in Delta image calc: {tend - tstart:.2f}s')\n",
    "    \n",
    "    delta_map_tot = curr_map * numerator * idenominator\n",
    "   \n",
    "    # You can also try to smooth the delta map\n",
    "    #curr_delta_map_tot = gaussian_filter(curr_delta_map_tot, 0.5)\n",
    "        \n",
    "    # zero our bad exposure regions\n",
    "    delta_map_tot[bad_expo] = 0\n",
    "\n",
    "    # should never happen\n",
    "    delta_map_tot[np.isnan(delta_map_tot)] = 0\n",
    "            \n",
    "    # convolve delta image\n",
    "    #print(f'Convolving Delta image, iteration {its}')\n",
    "    #tstart = time.time()\n",
    "    conv_delta_map_tot = convolve_fast(sky_response_scaled, delta_map_tot,\n",
    "                                    sky_response_scaled.shape[0], sky_response_scaled.shape[3],\n",
    "                                    n_b, n_l)\n",
    "    #tend = time.time()\n",
    "    #print(f'Time in convolution: {tend - tstart:.2f}s')\n",
    "\n",
    "    # Find maximum acceleration parameter to multiply delta image with\n",
    "    # so that the total image is still positive everywhere.\n",
    "    # If there are no negative entries in delta_map_tot_old, there is no upper bound on the\n",
    "    # acceleration.  Original code used a value of ~10000 in this case.  If we use much larger\n",
    "    # value, RL seems to oscillate rather than converging smoothly and gives a worse final\n",
    "    # likelihood (observed on the Point_Sources notebook, which is the only one with this issue).\n",
    "    assert np.min(curr_map) >= 0, \"current map contains negative entries!\"\n",
    "    neg = delta_map_tot < 0\n",
    "    if not neg.any():\n",
    "        afl = 10000\n",
    "    else:\n",
    "        afl = int(np.floor(np.min(-afl_scl * curr_map[neg] / delta_map_tot[neg]))) \n",
    "        afl = min(afl, 10000)\n",
    "    \n",
    "    print('Maximum acceleration parameter found: ', afl/afl_scl)\n",
    "\n",
    "    # fit:\n",
    "    \n",
    "    conv_sky = np.concatenate([[curr_expectation],[conv_delta_map_tot/afl_scl]])\n",
    "    \n",
    "    mu_Abg = fitted_bg    # can play with this\n",
    "    sigma_Abg = fitted_bg # can play with this\n",
    "    mu_flux = np.array([1,afl/2])\n",
    "    sigma_flux = np.array([1e-2,afl])\n",
    "\n",
    "    init_params =  (jnp.ones(Ncuts) * fitted_bg, jnp.array([1, afl/2.]))\n",
    "    \n",
    "    acceleration_factor_limit = afl * 0.95\n",
    "    lower_bounds = (jnp.ones(Ncuts) * 1e-8,    jnp.ones(2) * 1e-8)\n",
    "    upper_bounds = (jnp.ones(Ncuts) * jnp.inf, jnp.ones(2) * acceleration_factor_limit)\n",
    "    \n",
    "    #print('Optimizing bg parameters')\n",
    "    #tstart = time.time()\n",
    "    res = optimizer.run(init_params, bounds=(lower_bounds, upper_bounds),\n",
    "                        data=(conv_sky, model_bg, idx_arr, y,\n",
    "                              mu_flux, sigma_flux, mu_Abg, sigma_Abg))\n",
    "    #tend = time.time()\n",
    "    #print(f'Time in optimizer: {tend - tstart:.2f}s')\n",
    "\n",
    "    if not res.state.success:\n",
    "        print(\"*** Optimizer failed! rerun with options = { 'disp': True } to see error messages\")\n",
    "       \n",
    "        # proceed with a safe acceleration <= 1 (safe = new map does not go negative at any pixel)\n",
    "        print(\"proceeding with a safe acceleration parameter\")\n",
    "        accScale = np.minimum(1., acceleration_factor_limit)\n",
    "    else:\n",
    "        # save values\n",
    "        #print(f'Saving new map, and fitted parameters, iteration {its}')\n",
    "        intermediate_lp[its-1] = -res.state.fun_val\n",
    "        \n",
    "        newAbg, newflux = res.params\n",
    "        newAcc = newflux[1]\n",
    "        bg_pars[its-1,:] = newAbg\n",
    "        acc_par[its-1]   = newAcc\n",
    "\n",
    "        accScale = float(newAcc)/afl_scl\n",
    "    \n",
    "    # plot each iteration's map and its delta map here to match previous impl's behavior\n",
    "    # (not required, but nice to see how the algorithm is doing)\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(121)\n",
    "    plt.pcolormesh(L_ARRg,B_ARRg,np.roll(curr_map, axis=1, shift=0)) \n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.pcolormesh(L_ARRg,B_ARRg,np.roll(delta_map_tot, axis=1, shift=0)) \n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    # make new map as old map plus scaled delta map\n",
    "    curr_map += accScale * delta_map_tot\n",
    "    \n",
    "    # setting the map to zero where we selected a bad exposure (we didn't, but to keep it general)\n",
    "    curr_map[bad_expo] = 0\n",
    "    \n",
    "    # check for each pixel to be finite\n",
    "    curr_map[np.isnan(curr_map)] = 0\n",
    "\n",
    "    # save map\n",
    "    map_iterations[its,:,:] = curr_map \n",
    "\n",
    "    # make new expectation as old expectation plus scaled conv_delta map\n",
    "    curr_expectation += accScale * conv_delta_map_tot\n",
    "\n",
    "    # expectation (in data space) is the image (expectation) plus the background (model_bg)\n",
    "    curr_expectation_tot = curr_expectation + model_bg \n",
    "\n",
    "    # calculate likelihood of current total expectation\n",
    "    map_likelihoods[its] = cashstat(dataset, curr_expectation_tot)\n",
    "\n",
    "    # how much did the MAP likelihood improve since the prior iteration?\n",
    "    dml = np.abs((map_likelihoods[its] - map_likelihoods[its-1])/map_likelihoods[its-1])\n",
    "\n",
    "    print(f\"After iteration {its}: MAP likelihood = {map_likelihoods[its]:.2f}, rel. change = {dml:.2e}\")\n",
    "\n",
    "    if dml < ltol:\n",
    "        print(f\"MAP likelihood change was less than {ltol} -- terminating\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the fitted background parameter and the map flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(range(its), [i[0] for i in bg_pars[:its]], '.-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('BG params]')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "map_fluxes = np.zeros(its+1)\n",
    "for i in range(its+1):\n",
    "    map_fluxes[i] = np.sum(map_iterations[i,:,:]*domegaMap)\n",
    "    \n",
    "plt.plot(map_fluxes[:its],'o-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Flux')# [ph/keV]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did the algorithm converge? Look at the likelihoods.\n",
    "intermediate_lp: Fit likelihoods, i.e. fit quality\n",
    "\n",
    "map_likelihoods: likelihood of maps (vs. initial i.e. basically only background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(np.arange(its), intermediate_lp[:its], '.-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('likelihood (intermediate_lp)')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(range(its+1), map_likelihoods[:its+1], '.-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('likelihood (map_likelihoods)')\n",
    "\n",
    "print(f'final MAP likelihood = {map_likelihoods[its]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in contours from a real tracer of $^{26}$Al to plot over our RL image\n",
    "Read in the DIRBE 240$\\mu$m map (a far-infrared survey considered to be a good tracer of $^{26}$Al emission) as a FITS file, rebin it to $6^{\\circ} \\times 6^{\\circ}$ resolution, and plot contours over the RL iterations below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_ndarray(ndarray, new_shape, operation=\"sum\"):\n",
    "    \"\"\"\n",
    "    Bins an ndarray in all axes based on the target shape, by summing or\n",
    "        averaging.\n",
    "    Number of output dimensions must match number of input dimensions.\n",
    "    Example\n",
    "    -------\n",
    "    >>> m = np.arange(0,100,1).reshape((10,10))\n",
    "    >>> n = bin_ndarray(m, new_shape=(5,5), operation=â€˜sumâ€™)\n",
    "    >>> print(n)\n",
    "    [[ 22  30  38  46  54]\n",
    "     [102 110 118 126 134]\n",
    "     [182 190 198 206 214]\n",
    "     [262 270 278 286 294]\n",
    "     [342 350 358 366 374]]\n",
    "    \"\"\"\n",
    "    if not operation.lower() in ['sum', 'mean', 'average', 'avg']:\n",
    "        raise ValueError(\"Operation {} not supported.\".format(operation))\n",
    "    if ndarray.ndim != len(new_shape):\n",
    "        raise ValueError(\"Shape mismatch: {} -> {}\".format(ndarray.shape,\n",
    "                                                           new_shape))\n",
    "    compression_pairs = [(d, c//d) for d, c in zip(new_shape,\n",
    "                                                   ndarray.shape)]\n",
    "    flattened = [l for p in compression_pairs for l in p]\n",
    "    ndarray = ndarray.reshape(flattened)\n",
    "    for i in range(len(new_shape)):\n",
    "        if operation.lower() == \"sum\":\n",
    "            ndarray = ndarray.sum(-1*(i+1))\n",
    "        elif operation.lower() in [\"mean\", \"average\", \"avg\"]:\n",
    "            ndarray = ndarray.mean(-1*(i+1))\n",
    "    return ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirbe_1deg = fits.open('SPI_DIRBE_Orig_Knoedl_240um.fits')\n",
    "data = dirbe_1deg[2].data\n",
    "\n",
    "# interpolate to (360, 720)\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "new_dims = []\n",
    "for original_length, new_length in zip(data.shape, (360, 720)):\n",
    "    new_dims.append(np.linspace(0, original_length-1, new_length))\n",
    "    \n",
    "coords = np.meshgrid(*new_dims, indexing='ij')\n",
    "B = map_coordinates(data, coords)\n",
    "B.shape\n",
    "\n",
    "dirbe_6deg_6deg = bin_ndarray(B, (30, 60), 'mean')\n",
    "dirbe_6deg_6deg = np.flip(dirbe_6deg_6deg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the image!\n",
    "You can loop over all iterations to make a GIF or just show one iteration (usually the final iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import Video\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "from matplotlib import animation\n",
    "\n",
    "from matplotlib import colors\n",
    "\n",
    "from scipy.ndimage import gaussian_filter as smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an image to plot\n",
    "idx = its\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deg2rad = np.pi/180\n",
    "\n",
    "# Choose a color map like viridis (matplotlib default), nipy_spectral, twilight_shifted, etc. Not jet.\n",
    "cmap = plt.get_cmap('viridis') \n",
    "\n",
    "# Bad exposures will be gray\n",
    "cmap.set_bad('lightgray')\n",
    "\n",
    "\n",
    "##################\n",
    "# Select here which pixels should be gray\n",
    "map_iterations_nan = np.copy(map_iterations)\n",
    "\n",
    "# Select also non-zero exposures here to be gray (avoiding the edge effects)\n",
    "# You can play with this. Most success in testing with 1e4, 1e3\n",
    "bad_expo = np.where(expo_map/domegaMap <= 1e3) \n",
    "\n",
    "map_iterations_nan[:, bad_expo[0], bad_expo[1]] = np.nan\n",
    "#################    \n",
    "\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(10.24,7.68), subplot_kw={'projection':'aitoff'}, nrows=1, ncols=1)\n",
    "\n",
    "ax.set_xticks(np.array([-120,-60,0,60,120])*deg2rad)\n",
    "ax.set_xticklabels([r'$-120^{\\circ}$'+'\\n',\n",
    "                            r'$-60^{\\circ}$'+'\\n',\n",
    "                            r'$0^{\\circ}$'+'\\n',\n",
    "                            r'$60^{\\circ}$'+'\\n',\n",
    "                            r'$120^{\\circ}$'+'\\n'])\n",
    "ax.tick_params(axis='x', colors='orange')\n",
    "\n",
    "ax.set_yticks(np.array([-60,-30,0,30,60])*deg2rad)\n",
    "ax.tick_params(axis='y', colors='orange')\n",
    "\n",
    "plt.xlabel('Gal. Lon. [deg]')\n",
    "plt.ylabel('Gal. Lat. [deg]')\n",
    "\n",
    "\n",
    "# Optionally plot contours from the DIRBE 240um image\n",
    "levels = [np.max(dirbe_6deg_6deg)*0.05, np.max(dirbe_6deg_6deg)*0.1,\n",
    "          np.max(dirbe_6deg_6deg)*0.5, np.max(dirbe_6deg_6deg)*0.8]\n",
    "plt.contour(L_ARR, B_ARR, dirbe_6deg_6deg, levels=levels, colors='white', alpha=1)\n",
    "\n",
    "\n",
    "# \"ims\" is a list of lists, each row is a list of artists to draw in the\n",
    "# current frame; here we are just animating one artist, the image, in\n",
    "# each frame\n",
    "ims = []\n",
    "\n",
    "\n",
    "# If you want to make a GIF of all iterations:\n",
    "#for i in range(iterations):\n",
    "\n",
    "# If you only want to plot one image:\n",
    "for i in [idx]:\n",
    "\n",
    "    ttl = plt.text(0.5, 1.01, f'RL iteration {i}', horizontalalignment='center', \n",
    "                   verticalalignment='bottom', transform=ax.transAxes)\n",
    "    \n",
    "    # Either gray-out bad exposure (map_iterations_nan) or don't mask (map_iterations)\n",
    "    # Masking out bad exposure \n",
    "    #image = map_iterations_nan[i,:, :]\n",
    "    image = map_iterations[i, :, :]\n",
    "\n",
    "    \n",
    "    img = ax.pcolormesh(L_ARRg,B_ARRg,\n",
    "                        \n",
    "                        # Can shift the image along longitude. Here, no shift.\n",
    "                        np.roll(image, axis=1, shift=0),\n",
    "            \n",
    "                        # Optionally smooth with gaussian filter\n",
    "                        #smooth(np.roll(image, axis=1, shift=0), 0.75/pixel_size),\n",
    "                        \n",
    "                        cmap=plt.cm.viridis,\n",
    "                        \n",
    "                        # Optionally set the color scale. Default: linear\n",
    "                        #norm=colors.PowerNorm(0.33)\n",
    "                       )\n",
    "    ax.grid()\n",
    "    \n",
    "    ims.append([img, ttl])\n",
    "\n",
    "cbar = fig.colorbar(img, orientation='horizontal')\n",
    "cbar.ax.set_xlabel('[Arbitrary Units]')\n",
    "    \n",
    "\n",
    "# Can save a sole image as a PDF \n",
    "#plt.savefig(data_dir + f'images/26Al_RL_image_iteration{idx}.pdf', bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "# # Can save all iterations as a GIF\n",
    "# ani = animation.ArtistAnimation(fig, ims, interval=200, blit=True, repeat_delay=0)\n",
    "# ani.save(f'/home/jacqueline/26Al_RL_image_{idx}iterations.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do we see?\n",
    "\n",
    "As expected, we observe extended $^{26}$ Al emission along the Galactic Plane. There is concentrated emission in the Inner Galaxy. The RL algorithm therefore behaves as expected, placing photons simulated along the DIRBE 240$\\mu$m template map within the contours of this chosen tracer of $^{26}$Al emission.\n",
    "\n",
    "Given that only $\\sim$100 $^{26}$Al photons were detected during the COSI-balloon flight ($3.7\\sigma$ significance, [Beechert et al. 2022](https://iopscience.iop.org/article/10.3847/1538-4357/ac56dc/meta)), imaging the emission at its true flux instead of 10x strength would likely result in only imaging artifacts. Consider the following calculation. For $n$ spatial bins each with measurement significance $n_i$, the total significance of a measurement is \n",
    "\n",
    "$s = (\\sum_{i = 1}^{n} s_i^2)^{1/2}$.\n",
    "\n",
    "Even requiring only a weak $2\\sigma$ measurement in each bin, the maximum $n$ number of bins for a total $3.7\\sigma$ measurement is approximately 3.4. In other words, a $3.7\\sigma$ significant measurement distributed across the broad, diffuse $^{26}$Al emission which spans the Galactic Plane would result in few spatial bins with discernable significance. \n",
    "\n",
    "Below, we fit the simulated 10x flux image with a 2-D Gaussian as a demonstration of efforts to characterize the morphology of Galactic $^{26}$Al. \n",
    "\n",
    "Future data challenges will image $^{26}$Al as seen with the COSI satellite. Increased observation time, increased effective area, finer angular resolution, and observations at high Galactic latitudes (extending above and below the Galactic Plane) have great potential to advance understanding of this radioisotope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a 2D Gaussian to the emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_2d(xtuple, A, x0, y0, sigma_x, sigma_y, theta):\n",
    "    # theta: rotate the blob by positive, counterclockwise angle theta\n",
    "    (x, y) = xtuple\n",
    "    x0 = float(x0)\n",
    "    y0 = float(y0)\n",
    "    a = np.cos(theta)**2/(2*sigma_x**2) + np.sin(theta)**2/(2*sigma_y**2)\n",
    "    b = np.sin(2*theta)/(4*sigma_x**2) - np.sin(2*theta)/(4*sigma_y**2)\n",
    "    c = np.sin(theta)**2/(2*sigma_x**2) + np.cos(theta)**2/(2*sigma_y**2)\n",
    "    tot = A*np.exp( -( a*(x-x0)**2 + 2*b*(x-x0)*(y-y0) + c*(y-y0)**2 ) )\n",
    "    return tot.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "initial_guess = (2, 0, 0, 10, 10, 4)\n",
    "x = L_ARRg[:-1, :-1]\n",
    "y = B_ARRg[:-1, :-1]\n",
    "z = map_iterations_nan[idx, :, :]\n",
    "nan = np.isnan(z)\n",
    "z[nan] = 0.\n",
    "popt, pcov = opt.curve_fit(gauss_2d, (x, y), z.ravel(), p0=initial_guess, maxfev=5000)\n",
    "\n",
    "im_fitted = gauss_2d((x, y), *popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10.24, 7.68), subplot_kw={'projection':'aitoff'}, nrows=1, ncols=1)\n",
    "\n",
    "ax.set_xticks(np.array([-120, -60, 0, 60, 120])*deg2rad)\n",
    "ax.tick_params(axis='x', colors='orange')\n",
    "ax.set_xticklabels([r'$-120^{\\circ}$'+'\\n',\n",
    "                            r'$-60^{\\circ}$'+'\\n',\n",
    "                            r'$0^{\\circ}$'+'\\n',\n",
    "                            r'$60^{\\circ}$'+'\\n',\n",
    "                            r'$120^{\\circ}$'+'\\n'])\n",
    "ax.set_yticks(np.array([-60,-30,0,30,60])*deg2rad)\n",
    "ax.tick_params(axis='y', colors='orange')\n",
    "\n",
    "ax.set_xlabel('Gal. Lon. [deg]')\n",
    "ax.set_ylabel('Gal. Lat. [deg]')\n",
    "\n",
    "# Plot original image\n",
    "ax.pcolormesh(L_ARRg, B_ARRg, z.reshape(len(x), len(x[0])), cmap=plt.cm.viridis)\n",
    "\n",
    "# Plot contours\n",
    "num_contours = 2\n",
    "levels = [np.max(im_fitted)*0.05, np.max(im_fitted)*0.1,\n",
    "          np.max(im_fitted)*0.5, np.max(im_fitted)*0.8]\n",
    "\n",
    "plt.contour(L_ARR, B_ARR, im_fitted.reshape(len(x), len(x[0])), levels=levels, colors='white')\n",
    "\n",
    "cbar = fig.colorbar(img, orientation='horizontal')\n",
    "#cbar.ax.set_xlabel(r'Flux [10$^{-2}$ ph cm$^{-2}$ s$^{-1}$]')\n",
    "    \n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A:', popt[0])\n",
    "print('x0 [deg]:', popt[1]*180/np.pi)\n",
    "print('y0 [deg]:', popt[2]*180/np.pi)\n",
    "print('sigma_x [deg]:', popt[3]*180/np.pi, '--> FWHM_x [deg]:', 2*np.sqrt(2*np.log(2))*popt[3]*180/np.pi)\n",
    "print('sigma_y [deg]:', popt[4]*180/np.pi, '--> FWHM_y [deg]:', 2*np.sqrt(2*np.log(2))*popt[4]*180/np.pi)\n",
    "print('theta [deg]:', popt[5]*180/np.pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-cosi-python-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
