{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to 511 keV imaging with COSIpy-classic\n",
    "In this notebook, we'll use a Richardson-Lucy deconvolution algorithm to image 511 keV emission from the center of the Milky Way Galaxy. This analysis requires significant computer memory (>50 GB), so you may want to use a more resource-intensive computer for this work. Please refer to the README for additional information on each step of the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages\n",
    "We will need to import the cosipy-classic functions from COSIpy_dc1.py, response_dc1, and COSIpy_tools_dc1, as well as some other standard Python packages [Note: we need recent versions of Numba and Jaxopt for this version of the notebook - JDB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from COSIpy_dc1 import *\n",
    "import response_dc1\n",
    "from COSIpy_tools_dc1 import *\n",
    "from numba import jit, njit, prange, set_num_threads\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import pickle\n",
    "\n",
    "# set parallelism for whole notebook\n",
    "set_num_threads(8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For the modified RL algorithm implemented here, we need to define a jaxopt objective function that fits background plus two images (the current image plus a delta image given by the RL formalism)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.config\n",
    "import jax.numpy as jnp\n",
    "import jax.scipy.stats as jstats\n",
    "import jaxopt\n",
    "\n",
    "# to better match Stan's behavior\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "\n",
    "# objective function for MLE\n",
    "def objective(params, data):\n",
    "        (Abg, flux) = params\n",
    "        (conv_sky, bg_model, bg_idx_arr, y, mu_flux, sigma_flux, mu_Abg, sigma_Abg) = data\n",
    " \n",
    "        M = Abg[bg_idx_arr[:,None]] * bg_model + jnp.sum(flux[:,None,None] * conv_sky, axis=0)\n",
    "\n",
    "        # ensure that we don't accidentally use negative Possion means, which blows up likelihood\n",
    "        M = jnp.maximum(M, 0)\n",
    "        \n",
    "        lp = jnp.sum(jstats.poisson.logpmf(y, M), axis=None) + \\\n",
    "             jnp.sum(jstats.norm.logpdf(flux, mu_flux, sigma_flux)) + \\\n",
    "             jnp.sum(jstats.norm.logpdf(Abg, mu_Abg, sigma_Abg))\n",
    "        \n",
    "        return -lp  # minimize to maximize LL\n",
    "\n",
    "#opt = { 'disp': True }\n",
    "optimizer = jaxopt.ScipyBoundedMinimize(fun=objective, method=\"l-bfgs-b\", tol=1e-10)#, options=opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define file names\n",
    "This file contains the 10X flux 511 keV simulation and Ling BG. \n",
    "\n",
    "You can optionally image only 511 keV (without background) by changing this file to the 511 keV-only simulation. You will have to adjust the RL algorithm parameters later in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data_products' # directory containing data & response files\n",
    "filename = 'GC511_10xFlux_and_Ling.inc1.id1.extracted.tra.gz'# 511 keV with Ling BG\n",
    "response_filename = data_dir + '/511keV_imaging_response.npz' # detector response\n",
    "background_filename = data_dir + '/Scaled_Ling_BG_1x.npz' # background response\n",
    "background_mode = 'from file'\n",
    "\n",
    "pklfname = \"511keV.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read simulation and define analysis object\n",
    "Read in the data set and create the main cosipy-classic â€œanalysis1\" object, which provides various functionalities to study the specified file. This cell usually takes a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    analysis1 = pickle.load(open(pklfname,'rb'))\n",
    "    \n",
    "except:\n",
    "    print(\"loading analysis dataset\")\n",
    "    analysis1 = COSIpy(data_dir, filename)\n",
    "    analysis1.read_COSI_DataSet()\n",
    "    with open(pklfname, 'wb') as f:\n",
    "        pickle.dump(analysis1, f)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bin the data\n",
    "Calling \"get_binned_data()\" may take several minutes, depending on the size of the dataset and the number of bins. Keep an eye on memory here: if your time bins are very small, for example, this could be an expensive operation.\n",
    "\n",
    "As currently written, \"get_binned_data()\" uses about **4 GB memory**. [But this was reduced by at least half by storing the bin counts as ints, and by replacing lists with arrays - JDB.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the bin sizes\n",
    "Delta_T = 1800 # time bin size in seconds\n",
    "energy_bin_edges = np.array([501, 521]) # as defined in the response\n",
    "pixel_size = 6. # as defined in the response\n",
    "\n",
    "analysis1.dataset.time_binning_tags_fast(time_bin_size=Delta_T)\n",
    "analysis1.dataset.init_binning(energy_bin_edges=energy_bin_edges, pixel_size=pixel_size) # initiate the binning\n",
    "analysis1.dataset.get_binned_data_fast() # bin data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine the shape of the binned data.\n",
    "The binned data are contained in \"analysis1.dataset.binned_data.\" This is a 4-dimensional object representing the 5 dimensions of the Compton data space: (time, energy, $\\phi$, FISBEL).\n",
    "\n",
    "The number of bins in each dimension are shown by calling \"shape.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"time, energy, phi, fisbel\")\n",
    "print(analysis1.dataset.binned_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can print the width of each time bin and the total time\n",
    "print(analysis1.dataset.times.times_wid)\n",
    "print(analysis1.dataset.times.total_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot raw spectrum & light curve\n",
    "For a single energy bin, the spectrum is necessarily a top hat in the sole non-zero bin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis1.dataset.plot_lightcurve()\n",
    "\n",
    "analysis1.dataset.plot_raw_spectrum()\n",
    "plt.xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the pointing object with the cosipy pointing class.\n",
    "This may also take several minutes to run.  [Down to a few seconds after acceleration, even without parallelism - JDB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# definition of pointings (balloon stability + Earth rotation)\n",
    "pointing1 = Pointing(dataset=analysis1.dataset,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define the BG model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ling BG simulation to model atmospheric background\n",
    "background1 = BG(dataset=analysis1.dataset,mode=background_mode,filename=background_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in the Response Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 511 keV response\n",
    "rsp = response_dc1.SkyResponse(filename=response_filename,pixel_size=pixel_size) # read in detector response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the shape of the data space\n",
    "The shape of the response spans (Galactic latitude $b$, Galactic longitude $\\ell$, Compton scattering angle $\\phi$,  FISBEL, energy). There is 1 energy bin for the 511 keV response (\"analysis1.dataset.energies.n_energy_bins\"). This is why there is no fifth dimension for the energy printed below. The shape of the data and background objects span (time, Compton scattering angle, FISBEL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rsp.rsp.response_grid_normed_efinal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.shape(analysis1.dataset.binned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(background1.bg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imaging Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a grid on the sky to make images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenient variable for deg --> radian conversion\n",
    "deg2rad = np.pi/180.\n",
    "\n",
    "# We define our sky-grid on a regular (pixel_size x pixel_size) grid for testing (later finer grid)\n",
    "binsize = pixel_size\n",
    "\n",
    "# Number of pixels in l and b\n",
    "n_l = int(360/binsize)\n",
    "n_b = int(180/binsize)\n",
    "\n",
    "# Galactic coordiantes: l and b pixel edges\n",
    "l_arrg = np.linspace(-180, 180, n_l+1)\n",
    "b_arrg = np.linspace(-90, 90, n_b+1)\n",
    "\n",
    "# Making a grid\n",
    "L_ARRg, B_ARRg = np.meshgrid(l_arrg, b_arrg)\n",
    "\n",
    "# Choosing the centre points as representative\n",
    "l_arr = l_arrg[0:-1] + binsize/2\n",
    "b_arr = b_arrg[0:-1] + binsize/2\n",
    "L_ARR, B_ARR = np.meshgrid(l_arr, b_arr)\n",
    "\n",
    "# Define solid angle for each pixel for normalisations later\n",
    "domega = (binsize * deg2rad * np.diff(np.sin(np.deg2rad(b_arrg)))).repeat(n_l).reshape(n_b, n_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert sky grid to zenith/azimuth pairs for all pointings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the zeniths and azimuths on that grid for all times\n",
    "zensgrid,azisgrid = zenaziGrid_fast(pointing1.ypoins[:,0], pointing1.ypoins[:,1],\n",
    "                                    pointing1.xpoins[:,0], pointing1.xpoins[:,1],\n",
    "                                    pointing1.zpoins[:,0], pointing1.zpoins[:,1],\n",
    "                                    L_ARR.ravel(), B_ARR.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for next routines ... \n",
    "zensgrid = zensgrid.reshape(n_b, n_l, len(pointing1.xpoins))\n",
    "azisgrid = azisgrid.reshape(n_b, n_l, len(pointing1.xpoins))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get observation indices for non-zero bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an energy bin to analyze\n",
    "ebin = 0 # We only have one energy bin (501-521 keV), so the index is necessarily 0.\n",
    "nonzero_idx = background1.calc_this[ebin]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce the response dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sky_response_CDS = rsp.rsp.response_grid_normed_efinal.reshape(\n",
    "    n_b,\n",
    "    n_l,\n",
    "    analysis1.dataset.phis.n_phi_bins*\\\n",
    "    analysis1.dataset.fisbels.n_fisbel_bins, analysis1.dataset.energies.n_energy_bins)[:, :, nonzero_idx, ebin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduced response dimensions:\n",
    "# lat x lon x CDS\n",
    "sky_response_CDS.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to get the response of an image for arbitrary time binning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input assumptions:\n",
    "# - cdtpoins is sorted in nondecreasing order, as it results from a cumsum() call in the dataset construction code.\n",
    "# - tmin and tmax are lower and upper endpoints of time bins for Pointing data. Hence, tmax[i] = tmin[i+1].\n",
    "# - n_ph_dx gives the indices of the time bins with > 0 data items to be processed in the loop.  There are n_hours such nonempty bins.\n",
    "\n",
    "@njit(fastmath=True,parallel=True,nogil=True)\n",
    "def gir_loop(cdtpoins, tmin, n_ph_dx, n_hours, n_lat, n_lon, Response, weights, zens, azis):\n",
    "\n",
    "    image_response = np.empty((n_hours,n_lat,n_lon,Response.shape[2]), dtype=np.float32)\n",
    " \n",
    "    # Elts associated with jth time bin have values > tmin[j] and <= tmax[j].\n",
    "    # But there are no elts with value > tmax[j] and < tmin[j+1].\n",
    "    # Hence, indices for jth bin are bmins[j] .. bmins[j+1] - 1, inclusive.\n",
    "    bmins = np.searchsorted(cdtpoins, tmin[n_ph_dx], 'right') # least i with value > tmin\n",
    "\n",
    "    # Add end of cdtpoins array as sentinel to close last bin range.\n",
    "    bmins = np.append(bmins, cdtpoins.size)\n",
    "    \n",
    "    for c in prange(n_hours):\n",
    "        \n",
    "        acc = np.empty(Response.shape[2], dtype=np.float64)\n",
    "\n",
    "        for LAT in range(n_lat):\n",
    "            for LON in range(n_lon):\n",
    "                acc[:] = 0.\n",
    "                for v in range(bmins[c], bmins[c+1]):\n",
    "                    acc += Response[zens[LAT,LON,v], azis[LAT,LON,v],:] * weights[LAT,LON,v] # accumulate in 64 bits\n",
    "                image_response[c,LAT,LON,:] = acc\n",
    "\n",
    "    return image_response\n",
    "\n",
    "\n",
    "@jit\n",
    "def get_image_response_from_pixelhit_general(Response,zenith,azimuth,domega,dt,n_hours,binsize=6,cut=90,altitude_correction=False,al=None):\n",
    "    \"\"\"\n",
    "    Get Compton response from hit pixel for each zenith/azimuth vector(!) input.\n",
    "    Binsize determines regular(!!!) sky coordinate grid in degrees.\n",
    "\n",
    "    :param: zenith        Zenith positions of all points of predefined sky grid with\n",
    "                          respect to the instrument (in deg)\n",
    "    :param: azimuth       Azimuth positions of all points of predefined sky grid with\n",
    "                          respect to the instrument (in deg)\n",
    "    :param: domega        Latitude weighting of pixels on sky grid\n",
    "    :option: binsize      Default 6 deg (matching the sky dimension of the response). If set\n",
    "                          differently, make sure it matches the sky dimension as otherwise,\n",
    "                          false results may be returned\n",
    "    :option: cut          Threshold to cut the response calculation after a certain zenith angle.\n",
    "                          Default 90\n",
    "    :param: n_hours       Number of hours in cdxervation\n",
    "    :option: altitude_correction Default False: use interpolated transmission probability, normalised to 33 km and 500 keV,\n",
    "                          to modify number of expected photons as a function of altitude and zenith angle of cdxervation\n",
    "    :option: al           Altitude values according to dt from construct_pointings(); used of altitude_correction is set to True\n",
    "    \"\"\"\n",
    "\n",
    "    # assuming useful input:\n",
    "    # azimuthal angle is periodic in the range [0,360[\n",
    "    # zenith ranges from [0,180[\n",
    "\n",
    "    # and which pixel centre\n",
    "    #hit_pixel_z = (hit_pixel_zi + 0.5) * binsize\n",
    "    \n",
    "    zens = np.floor(zenith/binsize).astype(np.int32)\n",
    "    azis = np.floor(azimuth/binsize).astype(np.int32)\n",
    "        \n",
    "    nz = zenith.shape[2]\n",
    "\n",
    "    n_l = int(360/binsize)\n",
    "    n_b = int(180/binsize)\n",
    "    \n",
    "    # take care of regular grid by applying weighting with latitude \n",
    "    weights = domega.repeat(nz).reshape(n_b, n_l, nz) * dt\n",
    "    \n",
    "    # remove zeniths for which the pixel center is above the threshold\n",
    "    weights[zens > cut/binsize - 0.5] = 0.\n",
    "\n",
    "    # check for negative indices and remove\n",
    "    # NB: za_idx contains zeniths and azimuths, which are computed by zenazigrid.  The\n",
    "    # zeniths are outputs of arccos() and so lie in the range 0..2pi (before conversion\n",
    "    # # to degrees and pixel IDs, which cannot change the sign).  The azimuths are\n",
    "    # explicitly converted to be non-negative.  So this code is a no-op. - JDB\n",
    "    #weights[zens < 0] = 0.\n",
    "    #weights[azis < 0] = 0.\n",
    "    #zens[zens < 0] = 0\n",
    "    #azis[azis < 0] = 0\n",
    "            \n",
    "    #if altitude_correction == True:\n",
    "    #    altitude_response = return_altitude_response()\n",
    "    #else:\n",
    "    #    altitude_response = one_func\n",
    "\n",
    "    # get responses at pixels    \n",
    "    return gir_loop(pointing1.cdtpoins, \\\n",
    "                    analysis1.dataset.times.times_min, analysis1.dataset.times.n_ph_dx, \\\n",
    "                          n_hours, n_b, n_l, Response, weights, zens, azis)\n",
    "    \n",
    "    ## Original code replaced by above loop call\n",
    "    \n",
    "    #image_response = np.zeros((n_hours,n_lat,n_lon,Response.shape[2]))\n",
    "\n",
    "    #for c in tqdm(range(n_hours)):\n",
    "    #    cdx = np.where((pointing1.cdtpoins > analysis1.dataset.times.times_min[analysis1.dataset.times.n_ph_dx[c]]) &\n",
    "    #                   (pointing1.cdtpoins <= analysis1.dataset.times.times_max[analysis1.dataset.times.n_ph_dx[c]]))[0]\n",
    "    # \n",
    "    #    # this calculation is basically a look-up of the response entries. In general, weighting (integration) with the true shape can be introduced, however with a lot more computation time (Simpson's rule in 2D ...)\n",
    "    #    image_response[c,:,:,:] += np.sum(Response[za_idx[0,:,:,cdx],za_idx[1,:,:,cdx],:]*np.einsum('klij->iklj', weights[:,:,cdx,None])*dt[cdx,None,None,None],axis=0)#*altitude_weights[:,:,None]\n",
    "        \n",
    "    #return image_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the general response for the current data set\n",
    "This has to be done only once (for the data set).\n",
    "\n",
    "Takes ~20 minutes to run and ~60 GB memory! [Now down to 1-4 minutes (not sure why it varies so much!) and 35 GB - JDB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut = 90 \n",
    "sky_response_scaled = [] # clear out any old (large!) matrix if we are running this more than once\n",
    "sky_response_scaled = get_image_response_from_pixelhit_general(\n",
    "    Response=sky_response_CDS,\n",
    "    zenith=zensgrid,\n",
    "    azimuth=azisgrid,\n",
    "    domega=domega,\n",
    "    dt=pointing1.dtpoins,\n",
    "    n_hours=analysis1.dataset.times.n_ph,\n",
    "    binsize=pixel_size,\n",
    "    cut=cut)\n",
    "    #altitude_correction=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data-set-specific response dimensions\n",
    "# times x lat x lon x CDS\n",
    "sky_response_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exposure map\n",
    "i.e. the response weighted by time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mostly loop-nested version of computation.\n",
    "@njit(fastmath=True,parallel=True,nogil=True)\n",
    "def emap_fast(response, n_b, n_l):\n",
    "    expo_map = np.empty((n_b, n_l))\n",
    "    n_i = response.shape[0]\n",
    "    n_j = response.shape[3]\n",
    "\n",
    "    for x in prange(n_b):\n",
    "        for y in range(n_l):\n",
    "            expo_map[x,y] = 0\n",
    "            for i in range(n_i):\n",
    "                for j in range(n_j):\n",
    "                    expo_map[x,y] += response[i,x,y,j]\n",
    "                \n",
    "    return expo_map\n",
    "\n",
    "# original computation\n",
    "def emap(response, n_b, n_l):\n",
    "    expo_map = np.zeros((n_b, n_l))\n",
    "\n",
    "    for i in range(response.shape[0]):\n",
    "        expo_map += np.sum(response[i,:,:,:], axis=2)\n",
    "    return expo_map\n",
    " \n",
    "\n",
    "expo_map = emap_fast(sky_response_scaled, n_b, n_l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the exposure map weighted with the pixel size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(projection='aitoff')\n",
    "p = plt.pcolormesh(L_ARRg*deg2rad,B_ARRg*deg2rad,np.roll(expo_map/domega,axis=1,shift=0))\n",
    "plt.contour(L_ARR*deg2rad,B_ARR*deg2rad,np.roll(expo_map/domega,axis=1,shift=0),colors='black')\n",
    "plt.colorbar(p, orientation='horizontal')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up the RL algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function for a starting map for the RL deconvolution. We choose an isotropic map, i.e. all pixels on the sky are initialized with the same value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IsoMap(ll,bb,A0,binsize=pixel_size):\n",
    "    shape = np.ones(ll.shape)\n",
    "    norm = np.sum(shape*(binsize*np.pi/180)*(np.sin(np.deg2rad(bb+binsize/2)) - np.sin(np.deg2rad(bb-binsize/2))))\n",
    "    val = A0*shape/norm\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of time bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2h = analysis1.dataset.binned_data.shape[0]\n",
    "d2h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select only one energy bin (as above) for data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ebin: ',ebin)\n",
    "dataset = analysis1.dataset.binned_data[:,ebin,:,:].reshape(d2h,\n",
    "                                                            analysis1.dataset.phis.n_phi_bins*analysis1.dataset.fisbels.n_fisbel_bins)[:,nonzero_idx]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same for background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_model = background1.bg_model_reduced[ebin]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for consistency of data and background\n",
    "They must have the same dimensions. If not, the algorithm won't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape, background_model.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Set an initial guess for the background amplitude\n",
    "Feel free to play with this value, but here are suggestions informed by testing thus far:\n",
    "\n",
    "### If source+BG:\n",
    "We suggest setting \"fitted_bg\" to 0.9 or 0.99 when the loaded data/simulation (analysis1 object) contains both source and background. This is a rough estimate of the background contribution (90, 99%) to the entire data set.\n",
    "\n",
    "### If analyzing source only:\n",
    "When the analysis1 object does not contain background, we suggest setting this parameter to 1E-6, i.e. very close to zero background contribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_bg = np.array([0.99])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Richardson-Lucy algorithm\n",
    "\n",
    "## Individual steps are explained in the code.\n",
    "The steps follow the algorithm as outlined in [Knoedlseder et al. 1999](https://ui.adsabs.harvard.edu/abs/1999A%26A...345..813K/abstract). Refer to that paper for a mathematical description of the algorithm.\n",
    "\n",
    "The total memory used during these iterations is about 74 GB!! You might not be able to do much else with your machine while this is running. \n",
    "\n",
    "[Now down to < 15 secs per iteration. We run until the MAP likelihood converges, which takes around 20 iterations - JDB]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might not use this depending on if you choose to smooth the delta map\n",
    "\n",
    "from scipy.ndimage import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import convolve_fast, convdelta_fast\n",
    "import time\n",
    "\n",
    "# Experiment with these variables!\n",
    "#############################\n",
    "# initial map (isotropic flat, small value)\n",
    "map_init = IsoMap(L_ARR, B_ARR, 0.01)\n",
    "\n",
    "# number of RL iterations, Usually test with ~50 iterations, and we can get fully converged images with ~150 iterations. \n",
    "maxiters = 150\n",
    "\n",
    "# if MAP likelihood changes by less than this fraction in 1 iteration, terminate\n",
    "# NB: this is somewhat arbitrary and may be much too small in practice; I can't see\n",
    "# a difference in the images below 1e-7\n",
    "ltol = 1e-9\n",
    "\n",
    "# acceleration parameter\n",
    "afl_scl = 1000.\n",
    "#############################\n",
    "\n",
    "# Define regions of the sky that we actually cannot see\n",
    "# here we select everything, i.e. we have no bad exposure\n",
    "\n",
    "bad_expo = np.where(expo_map/domega <= 0)\n",
    "\n",
    "#############################\n",
    "\n",
    "## Define background model cuts, indices, and resulting number of cuts\n",
    "bg_cuts, idx_arr, Ncuts = background1.bg_cuts, background1.idx_arr, background1.Ncuts\n",
    " \n",
    "# temporary background model\n",
    "tmp_model_bg = np.zeros((d2h, background_model.shape[1]))\n",
    "\n",
    "for g in range(d2h):\n",
    "    tmp_model_bg[g,:] = background_model[g,:]*fitted_bg[idx_arr-1][g]\n",
    "\n",
    "## Save intermediate iterations: initialise arrays to save images and other parameters\n",
    "# maps per iteration\n",
    "map_iterations = np.zeros((maxiters, n_b, n_l))\n",
    "\n",
    "# likelihood of maps (vs. initial i.e. basically only background)\n",
    "map_likelihoods = np.zeros(maxiters)\n",
    "\n",
    "# store per-iter fit likelihoods, ie fit quality\n",
    "intermediate_lp = np.zeros(maxiters)\n",
    "\n",
    "# store per-iter acceleration parameters (lambda)\n",
    "acc_par = np.zeros(maxiters)\n",
    "\n",
    "# store per-iter fitted background parameters \n",
    "bg_pars = np.zeros((maxiters,Ncuts))\n",
    "\n",
    "\n",
    "## Zeroth iteration: copy initial map to become the 'old map' (see below)\n",
    "map_old = map_init\n",
    "\n",
    "# cf. Knoedlseder+1997 what the values denominator etc are\n",
    "# this is the response R summed over the CDS and the time bins\n",
    "denominator = expo_map\n",
    "   \n",
    "# convolve this map with the response\n",
    "print('Convolving with response (init expectation)')\n",
    "tstart = time.time()\n",
    "expectation_init = convolve_fast(sky_response_scaled, map_init,\n",
    "                                 sky_response_scaled.shape[0], sky_response_scaled.shape[3],\n",
    "                                 n_b, n_l)\n",
    "tend = time.time()\n",
    "print(f'Time in convolution: {tend - tstart:.2f}s')\n",
    "\n",
    "# set old expectation (in data space bins) to new expectation (convolved image)\n",
    "expectation_old = expectation_init\n",
    "\n",
    "# setting the map to zero where we selected a bad exposure (we didn't, but to keep it general)\n",
    "map_old[bad_expo] = 0\n",
    "\n",
    "# check for each pixel to be finite\n",
    "map_old[np.isnan(map_old)] = 0\n",
    "\n",
    "# save map from prior iteration\n",
    "map_iterations[0,:,:] = map_old \n",
    "\n",
    "# expectation (in data space) is the image (expectation_old) plus the background (tmp_model_bg)\n",
    "expectation_tot_old = expectation_old + tmp_model_bg \n",
    "\n",
    "# calculate likelihood of current total expectation\n",
    "map_likelihoods[0] = cashstat(dataset,expectation_tot_old)\n",
    "\n",
    "###########################################################\n",
    "###########################################################\n",
    "## here run over the number of iterations #################\n",
    "###########################################################\n",
    "## the time for the convolutions is very large ############\n",
    "## this can be 10 minutes (!) per iteration ###############\n",
    "## this should be tested for a few iterations #############\n",
    "## and then run overnight or similar ######################\n",
    "###########################################################\n",
    "###########################################################\n",
    "dml = np.inf\n",
    "for its in tqdm(range(1,maxiters)):\n",
    "    \n",
    "    if dml < ltol:\n",
    "        print(f\"MAP likelihood change was less than {ltol} -- terminating\")\n",
    "        break\n",
    "\n",
    "    # calculate numerator of RL algorithm\n",
    "   \n",
    "    print(f'Calculating Delta image, iteration {its}, numerator')\n",
    "    tstart = time.time()\n",
    "    W = dataset / expectation_tot_old - 1.\n",
    "    numerator = convdelta_fast(sky_response_scaled, W, n_b, n_l, W.shape[0], W.shape[1])\n",
    "    tend = time.time()\n",
    "    print(f'Time in Delta image calc: {tend - tstart:.2f}s')\n",
    "    \n",
    "    # calculate delta map (denominator scaled by fourth root to avoid exposure edge effects)\n",
    "    # You can try changing 0.25 to 0, 0.5, for example\n",
    "    delta_map_tot_old = (numerator/denominator)*map_old*(denominator)**0.25\n",
    "    \n",
    "    # Alternatively, you can also try to smooth it \n",
    "    #delta_map_tot_old = gaussian_filter(delta_map_tot_old, 0.5)\n",
    "    \n",
    "    #################################\n",
    "     \n",
    "    # check again for finite values and zero our bad exposure regions\n",
    "    delta_map_tot_old[bad_expo] = 0\n",
    "    delta_map_tot_old[np.isnan(delta_map_tot_old)] = 0\n",
    "    \n",
    "    # plot each iteration's map and its delta map \n",
    "    # (not required, but nice to see how the algorithm is doing)\n",
    "    plt.figure(figsize=(16,6))\n",
    "    plt.subplot(121)\n",
    "    plt.pcolormesh(L_ARRg,B_ARRg,np.roll(map_old, axis=1, shift=0)) \n",
    "    plt.colorbar()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    plt.pcolormesh(L_ARRg,B_ARRg,np.roll(delta_map_tot_old, axis=1, shift=0)) \n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "        \n",
    "    # convolve delta image\n",
    "    print(f'Convolving Delta image, iteration {its}')\n",
    "    tstart = time.time()\n",
    "    conv_delta_map_tot = convolve_fast(sky_response_scaled, delta_map_tot_old,\n",
    "                                  sky_response_scaled.shape[0], sky_response_scaled.shape[3],\n",
    "                                  n_b, n_l)\n",
    "    tend = time.time()\n",
    "    print(f'Time in convolution: {tend - tstart:.2f}s')\n",
    "\n",
    "    # find maximum acceleration parameter to multiply delta image with\n",
    "    # so that the total image is still positive everywhere\n",
    "    assert np.min(map_old) >= 0, \"map_old contains negative entries!\"\n",
    "\n",
    "    neg = delta_map_tot_old < 0\n",
    "\n",
    "    # If there are no negative entries in delta_map_tot_old, there is no upper bound on the\n",
    "    # acceleration.  Original code used a value of ~10000 in this case.  If we use much larger\n",
    "    # value, RL seems to oscillate rather than converging smoothly and gives a worse final\n",
    "    # likelihood (observed on the Point_Sources notebook, which is the only one with this issue).\n",
    "    if not neg.any():\n",
    "        afl = 10000\n",
    "    else:\n",
    "        afl = int(np.floor(np.min(-afl_scl * map_old[neg] / delta_map_tot_old[neg]))) \n",
    "        afl = min(afl, 10000)\n",
    "    \n",
    "    print('Maximum acceleration parameter found: ', afl/afl_scl)\n",
    "\n",
    "    # fit:\n",
    "    y = dataset.astype(int)\n",
    "\n",
    "    conv_sky = np.concatenate([[expectation_old],[conv_delta_map_tot/afl_scl]])\n",
    "    \n",
    "    mu_Abg = fitted_bg    # can play with this\n",
    "    sigma_Abg = fitted_bg # can play with this\n",
    "    mu_flux = np.array([1,afl/2])\n",
    "    sigma_flux = np.array([1e-2,afl])\n",
    "\n",
    "    init_params =  (jnp.ones(Ncuts) * fitted_bg, jnp.array([1, afl/2.]))\n",
    "    \n",
    "    acceleration_factor_limit = afl * 0.95\n",
    "    lower_bounds = (jnp.ones(Ncuts) * 1e-8,    jnp.ones(2) * 1e-8)\n",
    "    upper_bounds = (jnp.ones(Ncuts) * jnp.inf, jnp.ones(2) * acceleration_factor_limit)\n",
    "    \n",
    "    tstart = time.time()\n",
    "    res = optimizer.run(init_params, bounds=(lower_bounds, upper_bounds),\n",
    "                        data=(conv_sky, tmp_model_bg, idx_arr, y,\n",
    "                              mu_flux, sigma_flux, mu_Abg, sigma_Abg))\n",
    "    tend = time.time()\n",
    "    print(f'Time in optimizer: {tend - tstart:.2f}s')\n",
    "\n",
    "    if not res.state.success:\n",
    "        print(\"*** Optimizer failed! rerun with options = { 'disp': True } to see error messages\")\n",
    "       \n",
    "        # proceed with a safe acceleration <= 1 (safe = new map does not go negative at any pixel)\n",
    "        print(\"proceeding with a safe acceleration parameter\")\n",
    "        accScale = np.min(1., acceleration_factor_limit)\n",
    "    else:\n",
    "        # save values\n",
    "        print(f'Saving new map, and fitted parameters, iteration {its}')\n",
    "        intermediate_lp[its-1] = -res.state.fun_val\n",
    "        \n",
    "        newAbg, newflux = res.params\n",
    "        newAcc = newflux[1]\n",
    "        bg_pars[its-1,:] = newAbg\n",
    "        acc_par[its-1]   = newAcc\n",
    "\n",
    "        accScale = newAcc/afl_scl\n",
    "    \n",
    "    # make new map as old map plus scaled delta map (must copy out of Jax to make writable on next line)\n",
    "    map_new = np.array(map_old + accScale * delta_map_tot_old)\n",
    "    \n",
    "    # setting the map to zero where we selected a bad exposure (we didn't, but to keep it general)\n",
    "    map_new[bad_expo] = 0\n",
    "    \n",
    "    # check for each pixel to be finite\n",
    "    map_new[np.isnan(map_old)] = 0\n",
    "\n",
    "    # save map from prior iteration\n",
    "    map_iterations[its,:,:] = map_new \n",
    "\n",
    "    # make new expectation as old expectation plus scaled conv_delta map\n",
    "    expectation_new = np.array(expectation_old + accScale * conv_delta_map_tot)\n",
    "\n",
    "    # expectation (in data space) is the image (expectation_old) plus the background (tmp_model_bg)\n",
    "    expectation_tot_new = expectation_new + tmp_model_bg \n",
    "\n",
    "    # calculate likelihood of current total expectation\n",
    "    map_likelihoods[its] = cashstat(dataset, expectation_tot_new)\n",
    "\n",
    "    # how much did the MAP likelihood improve since the last iteration?\n",
    "    dml = np.abs((map_likelihoods[its] - map_likelihoods[its-1])/map_likelihoods[its-1])\n",
    "    \n",
    "    # swap maps\n",
    "    map_old = map_new\n",
    "    \n",
    "    # and expectations\n",
    "    expectation_old = expectation_new\n",
    "    expectation_tot_old = expectation_tot_new\n",
    "    \n",
    "    print(f\"After iteration {its}: MAP likelihood = {map_likelihoods[its]:.2f}, rel. change = {dml:.2e}\")\n",
    "    # and repeat\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the fitted background parameter and the map flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(range(its-1), [i[0] for i in bg_pars[:its-1]], '.-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('BG params]')\n",
    "\n",
    "\n",
    "plt.subplot(122)\n",
    "map_fluxes = np.zeros(its)\n",
    "for i in range(its):\n",
    "    map_fluxes[i] = np.sum(map_iterations[i,:,:]*domega)\n",
    "    \n",
    "plt.plot(map_fluxes[:its],'o-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Flux')# [ph/keV]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Did the algorithm converge? Look at the likelihoods.\n",
    "intermediate_lp: Fit likelihoods, i.e. fit quality\n",
    "\n",
    "map_likelihoods: likelihood of maps (vs. initial i.e. basically only background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,6))\n",
    "plt.subplot(121)\n",
    "plt.plot(np.arange(1, its), intermediate_lp[:its-1], '.-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('likelihood (intermediate_lp)')\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(range(its+1), map_likelihoods[:its+1], '.-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('likelihood (map_likelihoods)')\n",
    "\n",
    "print(f'final MAP likelihood = {map_likelihoods[its]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the image!\n",
    "You can loop over all iterations to make a GIF or just show one iteration (usually the final iteration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.display import Video\n",
    "\n",
    "from matplotlib import animation\n",
    "\n",
    "from matplotlib import colors\n",
    "\n",
    "from scipy.ndimage import gaussian_filter as smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose an image to plot\n",
    "idx = its\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a color map like viridis (matplotlib default), nipy_spectral, twilight_shifted, etc. Not jet.\n",
    "cmap = plt.get_cmap('viridis') \n",
    "\n",
    "# Bad exposures will be gray\n",
    "cmap.set_bad('lightgray')\n",
    "\n",
    "\n",
    "##################\n",
    "# Select here which pixels should be gray\n",
    "map_iterations_nan = np.copy(map_iterations)\n",
    "\n",
    "# Select also non-zero exposures here to be gray (avoiding the edge effects)\n",
    "# You can play with this. Most success in testing with 1e4, 1e3\n",
    "bad_expo = np.where(expo_map/domega <= 1e4) \n",
    "\n",
    "for i in range(maxiters):\n",
    "    map_iterations_nan[i, bad_expo[0], bad_expo[1]] = np.nan\n",
    "#################    \n",
    "\n",
    "\n",
    "# Set up the plot\n",
    "fig, ax = plt.subplots(figsize=(10.24,7.68), subplot_kw={'projection':'aitoff'}, nrows=1, ncols=1)\n",
    "\n",
    "ax.set_xticks(np.array([-120,-60,0,60,120])*deg2rad)\n",
    "ax.set_xticklabels([r'$-120^{\\circ}$'+'\\n',\n",
    "                            r'$-60^{\\circ}$'+'\\n',\n",
    "                            r'$0^{\\circ}$'+'\\n',\n",
    "                            r'$60^{\\circ}$'+'\\n',\n",
    "                            r'$120^{\\circ}$'+'\\n'])\n",
    "ax.tick_params(axis='x', colors='orange')\n",
    "\n",
    "ax.set_yticks(np.array([-60,-30,0,30,60])*deg2rad)\n",
    "ax.tick_params(axis='y', colors='orange')\n",
    "\n",
    "plt.xlabel('Gal. Lon. [deg]')\n",
    "plt.ylabel('Gal. Lat. [deg]')\n",
    "\n",
    "\n",
    "\n",
    "# \"ims\" is a list of lists, each row is a list of artists to draw in the\n",
    "# current frame; here we are just animating one artist, the image, in\n",
    "# each frame\n",
    "ims = []\n",
    "\n",
    "\n",
    "# If you want to make a GIF of all iterations:\n",
    "#for i in range(its):\n",
    "\n",
    "# If you only want to plot one image:\n",
    "for i in [idx]:\n",
    "\n",
    "    ttl = plt.text(0.5, 1.01, f'RL iteration {i}', horizontalalignment='center', \n",
    "                   verticalalignment='bottom', transform=ax.transAxes)\n",
    "    \n",
    "    # Either gray-out bad exposure (map_iterations_nan) or don't mask (map_iterations)\n",
    "    # Masking out bad exposure \n",
    "    #image = map_iterations_nan[i,:,:]\n",
    "    image = map_iterations[i,:,:]\n",
    "\n",
    "    \n",
    "    img = ax.pcolormesh(L_ARRg*deg2rad,B_ARRg*deg2rad,\n",
    "                        \n",
    "                        # Can shift the image along longitude. Here, no shift.\n",
    "                        np.roll(image, axis=1, shift=0),\n",
    "            \n",
    "                        # Optionally smooth with gaussian filter\n",
    "                        #smooth(np.roll(image, axis=1, shift=0), 0.75/pixel_size),\n",
    "                        \n",
    "                        cmap=plt.cm.viridis,\n",
    "                        \n",
    "                        # Optionally set the color scale. Default: linear\n",
    "                        #norm=colors.PowerNorm(0.33)\n",
    "                       )\n",
    "    ax.grid()\n",
    "    \n",
    "    ims.append([img, ttl])\n",
    "\n",
    "cbar = fig.colorbar(img, orientation='horizontal')\n",
    "cbar.ax.set_xlabel(r'[Arbitrary Units]')\n",
    "    \n",
    "\n",
    "# Can save a sole image as a PDF \n",
    "#plt.savefig(data_dir + f'images/511keV_RL_image_iteration{idx}.pdf', bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "# # Can save all iterations as a GIF\n",
    "# ani = animation.ArtistAnimation(fig, ims, interval=200, blit=True, repeat_delay=0)\n",
    "# ani.save(f'/home/jacqueline/511keV_RL_image_{idx}iterations.gif')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What do we see?\n",
    "\n",
    "We clearly see the \"bulge\" emission of positron-electron annihilation at the center of the Milky Way. This was also seen in the published image of real COSI-balloon flight data [(Siegert et al. 2020)](https://iopscience.iop.org/article/10.3847/1538-4357/ab9607/meta):\n",
    "\n",
    "<img width=\"600\" alt=\"Siegert_2020_COSI_511keV\" src=\"https://user-images.githubusercontent.com/33991471/196853486-68a90111-245b-442d-841c-756f47c9c14f.png\">\n",
    "\n",
    "\n",
    "The extended disk emission seen in the SPI image above is not visible here. This is expected; SPI saw about 1 photon per week from the disk and has over a decade of observation time. There is not enough data in the 46-day balloon flight to image the disk.\n",
    "\n",
    "However, we can still probe the emission morphology of the bulge by fitting a 2-D Gaussian, for example, to our simulated image. Constraining the parameters of this fit is important for modeling the physics (positron propogation, point sources of positrons, etc.) behind this enduring mystery.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit a 2D Gaussian to the emission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gauss_2d(xtuple, A, x0, y0, sigma_x, sigma_y, theta):\n",
    "    # theta: rotate the blob by positive, counterclockwise angle theta\n",
    "    # https://en.wikipedia.org/wiki/Gaussian_function#Two-dimensional_Gaussian_function\n",
    "    # NB: code actually rotates *clockwise* by theta because sign of b formula is inverted\n",
    "    (x, y) = xtuple\n",
    "    x0 = float(x0)\n",
    "    y0 = float(y0)\n",
    "    a = np.cos(theta)**2/(2*sigma_x**2) + np.sin(theta)**2/(2*sigma_y**2)\n",
    "    b = np.sin(2*theta)/(4*sigma_x**2)  - np.sin(2*theta)/(4*sigma_y**2)\n",
    "    c = np.sin(theta)**2/(2*sigma_x**2) + np.cos(theta)**2/(2*sigma_y**2)\n",
    "    tot = A*np.exp( -( a*(x-x0)**2 + 2*b*(x-x0)*(y-y0) + c*(y-y0)**2 ) )\n",
    "    return tot.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.optimize as opt\n",
    "initial_guess = (2, 0, 0, 10, 10, 2)\n",
    "x = (L_ARRg*deg2rad)[:-1, :-1]\n",
    "y = (B_ARRg*deg2rad)[:-1, :-1]\n",
    "z = map_iterations_nan[idx,:,:]\n",
    "z[np.isnan(z)] = 0.\n",
    "\n",
    " # added bounds and boosted feval max to improve convergence and give sensible angle theta - JDB\n",
    "popt, pcov = opt.curve_fit(gauss_2d, (x, y), z.ravel(), p0=initial_guess, \n",
    "                           bounds=([0,-np.inf,-np.inf,0,0,-np.pi],[np.inf,np.inf,np.inf,np.inf,np.inf,np.pi]),maxfev=10000)\n",
    "\n",
    "im_fitted = gauss_2d((x, y), *popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10.24,7.68),subplot_kw={'projection':'aitoff'},nrows=1,ncols=1)\n",
    "\n",
    "ax.set_xticks(np.array([-120,-60,0,60,120])*deg2rad)\n",
    "ax.tick_params(axis='x', colors='orange')\n",
    "ax.set_xticklabels([r'$-120^{\\circ}$'+'\\n',\n",
    "                            r'$-60^{\\circ}$'+'\\n',\n",
    "                            r'$0^{\\circ}$'+'\\n',\n",
    "                            r'$60^{\\circ}$'+'\\n',\n",
    "                            r'$120^{\\circ}$'+'\\n'])\n",
    "ax.set_yticks(np.array([-60,-30,0,30,60])*deg2rad)\n",
    "ax.tick_params(axis='y', colors='orange')\n",
    "\n",
    "ax.set_xlabel('Gal. Lon. [deg]')\n",
    "ax.set_ylabel('Gal. Lat. [deg]')\n",
    "\n",
    "# Plot original image\n",
    "ax.pcolormesh(L_ARRg*deg2rad, B_ARRg*deg2rad, z.reshape(len(x), len(x[0])), cmap=plt.cm.viridis)\n",
    "\n",
    "# Plot contours\n",
    "num_contours = 2\n",
    "levels = [np.max(im_fitted)*0.05, np.max(im_fitted)*0.1,\n",
    "          np.max(im_fitted)*0.5, np.max(im_fitted)*0.8]\n",
    "\n",
    "#plt.contour(x, y, im_fitted.reshape(len(x), len(x[0])), levels=num_contours, colors='w')\n",
    "\n",
    "plt.contour(L_ARR*deg2rad, B_ARR*deg2rad, im_fitted.reshape(len(x), len(x[0])), levels = levels, colors='white')\n",
    "\n",
    "cbar = fig.colorbar(img, orientation='horizontal')\n",
    "#cbar.ax.set_xlabel(r'Flux [10$^{-2}$ ph cm$^{-2}$ s$^{-1}$]')\n",
    "    \n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('A:', popt[0])\n",
    "print('x0 [deg]:', popt[1]*180/np.pi)\n",
    "print('y0 [deg]:', popt[2]*180/np.pi)\n",
    "print('sigma_x [deg]:', popt[3]*180/np.pi, '--> FWHM_x [deg]:', 2*np.sqrt(2*np.log(2))*popt[3]*180/np.pi)\n",
    "print('sigma_y [deg]:', popt[4]*180/np.pi, '--> FWHM_y [deg]:', 2*np.sqrt(2*np.log(2))*popt[4]*180/np.pi)\n",
    "print('theta [deg]:', popt[5]*180/np.pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-cosi-python-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
